# Machine learning benchmarks

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="ch20-figures/")
if(FALSE){
  knitr::knit("index.qmd")
}
```

Dans ce chapitre, nous allons explorer plusieurs visualisations qui compare la performance des algorithmes d’apprentissage artificiel, avec les packages de `mlr3`.

<!-- comment -->

* Nous commençons avec une explication de l’utilité des packages `mlr3` pour la comparaison des algorithmes d’apprentissage artificiel.
<!-- comment -->
* Nous définissons une comparaison sur plusieurs problèmes de classification binaire, avec plusieurs algorithmes d’apprentissage (plus proches voisins, modèle linaire, réseau de neurones).
<!-- comment -->
* Nous faisons des graphiques montrant les taux d’erreur et l’aire sous la courbe ROC.
<!-- comment -->
* Nous rajoutons l’interactivité aux courbes ROC, pour montrer plusieurs niveaux de détail (données, itération de validation croisée, seuil des faux positifs).
<!-- comment -->
* Nous visualisons des mesures de fidelité et d’erreur par rapport à l’ensemble de validation, pour vérifier si nous avons bien évité le surapprentissage et le sous-apprentissage.

<!-- comment -->

## Motivation pour `mlr3` {#motivation-mlr3}

<!-- comment -->

Il y a plusieurs packages R qui proposent des algorithmes d’apprentissage artificiel, par exemple :

<!-- comment -->

* `library(kknn)` fournit K Plus Proches Voisins (KPPV), que nous avons exploré dans le [chapitre 10](/ch10).
<!-- comment -->
* `library(glmnet)` fournit les modèles lineaires avec régularisation L1, commo le LASSO, que nous avons exploré dans le [chapitre 11](/ch11).
<!-- comment -->
* `library(torch)` fournit les réseaux des neurones, que nous avons exploré dans le [chapitre 18](/ch18).

<!-- comment -->

Pour obtenir des bons résultats de prédiction sur un jeu de données, nous voulons typiquement entraîner plusieurs algorithmes différents, car nous ne savons pas quel algorithme aura les meilleures performances dans ces données.
<!-- comment -->
Alors nous voulons utiliser plusieurs différents packages, pour ensuite analyser les résultats ensemble, pour déterminer quel algorithme donne les meilleures prédictions.
<!-- comment -->
Dans chaque package, les fonctions pour l’apprentissage et la prédiction ont différents noms, et différentes interfaces.
<!-- comment -->
Typiquement chaque package contient une fonction `predict()` qui calcule les prédictions, mais le type de sortie est différent entre packages (`factor` pour classe, `matrix` pour probabilités de chaque classe, et cætera).
<!-- comment -->
Comment adapter chaque jeu de données pour chaque package ?
<!-- comment -->
Et comment convertir les prédictions de chaque package, pour avoir un type de sortie uniforme, qui permet la comparaison de résultats ?
<!-- comment -->
Le package `mlr3` fournit plusieurs fonctions qui simplifient ce genre d’analyse, qu’on appelle « benchmark » [@bischl2024applied].

<!-- comment -->

* Chaque jeu de données est un `Task` qui rélie un tableau de données d’apprentissage avec quelques méta-données sur quelles colonnes seront utilisées pour différents rôles (entrée, sortie, et cætera).
<!-- comment -->
* Chaque algorithme d’apprentissage est un `Learner` qui fournit les prédictions avec type uniforme (matrice de probabilités pour chaque classe).

<!-- comment -->

Dans ce chapitre, nous expliquons l’utilisation d’`animint2` pour la visualisation interactive des résultats des différents algorithmes d’apprentissage sur différents jeux de données.
<!-- comment -->
Le code dans ce chapitre est une version modifiée du code qui fait des visualisations statiques [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Definition des données et algorithmes à comparer {#definition-comparaison}

<!-- comment -->

Dans `mlr3`, une comparaison « benchmark » comprend toutes les combinaisons de :

<!-- comment -->

* une liste de jeux de données (`Tasks`),
<!-- comment -->
* une liste d’algorithmes d’apprentissage (`Learners`),
<!-- comment -->
* et toutes les itérations d’une méthode d’échantillonage (`Resampling`).

<!-- comment -->

### Les jeux de données (`Tasks`) {#tasks}

<!-- comment -->

Nous commençons avec une liste de deux jeux de données.

```{r}
mlr.tasks <- c("spam","sonar")
task_list <- mlr3::tsks(mlr.tasks)
```

Ensuite, nous définissons une fonction pour télécharger un jeu de données depuis le site web du livre « Elements of Statistical Learning » [@Hastie2009].

```{r}
library(data.table)
prefix <- "https://hastie.su.domains/ElemStatLearn/datasets/"
cache.fread <- function(data.name, f){
  cache.dir <- file.path("data", data.name)
  dir.create(cache.dir, showWarnings=FALSE, recursive=TRUE)
  local.path <- file.path(cache.dir, f)
  if(!file.exists(local.path)){
    u <- paste0(prefix, f)
    download.file(u, local.path)
  }
  fread(local.path)
}
```

Ensuite, nous faisons un boucle pour télécharger trois jeux de données.
Pour chacun, on rajoute un `Task` de classification binaire (avec seulement les deux premières classes).

```r
data.sets <- c("vowel","waveform","zip")
for(data.name in data.sets){
  suffix <- if(data.name=="zip")".gz" else ""
  set.list <- list()
  for(predefined.set in c("test","train")){
    one.set.dt <- cache.fread(
      data.name,
      paste0(data.name, ".", predefined.set, suffix)
    )
    if("row.names" %in% names(one.set.dt)){
      one.set.dt[, row.names := NULL]
    }
    setnames(one.set.dt, old=names(one.set.dt)[1], new="y")
    set.list[[predefined.set]] <- one.set.dt
  }
  task.dt <- rbindlist(set.list)[
    y %in% unique(y)[1:2] #only first two classes.
  ][, y := factor(y)]
  one.task <- mlr3::TaskClassif$new(data.name, task.dt, target='y')
  task_list[[data.name]] <- one.task
}
task_list
```

Le résultat ci-dessus est une liste représentant 5 problèmes de classification binaire :

<!-- comment -->

* `spam` représente la classification des courriels (`spam` est pourriel non-désiré, `nonspam` est courriel désiré) à partir d’un vecteur de comtages de mots « bag of words ».
<!-- comment -->
* [`sonar`](https://rdrr.io/cran/mlbench/man/Sonar.html) représente la classification de métal et roche, à partir d’un vecteur de bandes de fréquences sonores.
<!-- comment -->
* `vowel` représente la classification des sons voyelles, à partir de dix variables calculées depuis un fichier audio.
<!-- comment -->
* `waveform` est un jeu de données de simulation [@Hastie2009].
<!-- comment -->
* `zip` représente la classification de chiffres écrits à la main, à partir d’une image de 16x16 pixels, en échelle de gris.

<!-- comment -->

### Les algorithmes d’apprentissage {#learners}

<!-- comment -->

Dans cette section, nous définissons les algorithmes d’apprentissage à utiliser avec chaque jeu de données.
<!-- comment -->
Le code ci-dessous commence avec une liste de deux algorithmes :

<!-- comment -->

* `cv_glmnet` est un modèle linaire, avec régularisation L1 qui minimise le taux d’erreur dans la validation croisée à 10 divisions.
<!-- comment -->
* `featureless` est le modèle qui n’utilise pas les variables d’entrée, et qui va toujours prédire la classe la plus fréquente dans les données d’apprentissage.

<!-- comment -->

```{r}
learner.list <- list(
  mlr3learners::LearnerClassifCVGlmnet$new()$configure(id="cv_glmnet"),
  mlr3::LearnerClassifFeatureless$new()$configure(id="featureless"))
```

Ensuite, nous rajoutons l’algorithme des plus proches voisins.

```{r}
knn_learner <- mlr3learners::LearnerClassifKKNN$new()
knn_learner$param_set$values$k <- paradox::to_tune(1, 40)
```

Le code ci-dessus précise les bornes pour `k`, le nombre de voisins, entre 1 et 40.
<!-- comment -->
Le code ci-dessous précise la méthode pour la séléction de `k`, soit recherche avec 10 valeurs, avec validation croisée à 3 divisions, maximisant l’aire sous la courbe ROC, soit en anglais « Area Under the Curve » ou « AUC ».
<!-- comment -->
Rémarquons que si on veut calculer l’AUC pour l’ensemble de validation, il faut préciser `predict_type="prob"`, ce qui fait que la prédiction est un nombre réel (et non seulement la classe la plus probable).

```{r}
kfoldcv <- mlr3::rsmp("cv")
kfoldcv$param_set$values$folds <- 3
knn_learner$predict_type <- "prob"
learner.list$knn <- mlr3tuning::auto_tuner(
  learner = knn_learner,
  tuner = mlr3tuning::tnr("grid_search"),
  resampling = kfoldcv,
  measure = mlr3::msr("classif.auc", minimize = FALSE))
```

Ensuite, nous rajoutons deux algorithmes qui utilise `library(torch)`.
<!-- comment -->
On utilise une fonction qui renvoye une liste d’opérateurs qui sera utilisé pour définir l’algorithme.

```{r}
measure_list <- mlr3::msrs(c("classif.logloss", "classif.auc"))
n.epochs <- 200
make_po_list <- function(...)list(
  mlr3pipelines::po("scale"),
  mlr3pipelines::po(
    "select",
    selector = mlr3pipelines::selector_type(c("numeric", "integer"))),
  mlr3torch::PipeOpTorchIngressNumeric$new(),
  ...,
  mlr3pipelines::po("nn_head"),
  mlr3pipelines::po(
    "torch_loss",
    mlr3torch::t_loss("cross_entropy")),
  mlr3pipelines::po(
    "torch_optimizer",
    mlr3torch::t_opt("sgd", lr=0.1)),
  mlr3pipelines::po(
    "torch_callbacks",
    mlr3torch::t_clbk("history")),
  mlr3pipelines::po(
    "torch_model_classif",
    batch_size = 10,
    patience=n.epochs,
    measures_valid=measure_list,
    measures_train=measure_list,
    predict_type="prob",
    epochs = paradox::to_tune(upper = n.epochs, internal = TRUE)))
```

Remarquons dans le code ci-dessus que `lr` est le taux d’apprentissage, ou « learning rate » en anglais.
<!-- comment -->
La fonction ci-dessous définit que

<!-- comment -->

* la moitié des données d’apprentissage comme ensemble de validation (`validate = 0.5`), et l’autre moitié est l’ensemble sous-entraînement (pour calculer les gradients).
<!-- comment -->
* le nombre d’époques est choisi en minimisant la première mesure dans la liste `measure_list` (perte logistique).

```{r}
make_torch_learner <- function(id,...){
  po_list <- make_po_list(...)
  graph <- Reduce(mlr3pipelines::concat_graphs, po_list)
  glearner <- mlr3::as_learner(graph)
  mlr3::set_validate(glearner, validate = 0.5)
  mlr3tuning::auto_tuner(
    learner = glearner,
    tuner = mlr3tuning::tnr("internal"),
    resampling = mlr3::rsmp("insample"),
    measure = mlr3::msr("internal_valid_score", minimize = TRUE),
    term_evals = 1, id = id, store_models = TRUE)
}
```

Le code ci-dessous rajoute deux algorithmes d’apprentissage.

```{r}
if(torch::torch_is_installed()){
  learner.list$linear <- make_torch_learner("torch_linear")
  learner.list$dense <- make_torch_learner(
    "torch_dense_50",
    mlr3pipelines::po(
      "nn_linear",
      out_features = 50),
    mlr3pipelines::po("nn_relu_1", inplace = TRUE))
}
```

### Calculer les résultats {#running-the-benchmark}

<!-- comment -->

Avant de calculer les résultats pour des algorithmes de classification, on fait `predict_type=prob` pour s’assurer que les prédictions soient des valeurs réelles (pour pouvoir calculer les mesures d’évaluation comme l’AUC).

```{r}
for(learner.i in seq_along(learner.list)){
  learner.list[[learner.i]]$predict_type <- "prob"
}
```

Ensuite, nous combinons les jeux de données, les algorithmes d’apprentissage, et la méthode d’échantillonage (validation croisée à 3 divisions).

```{r}
bench.grid <- mlr3::benchmark_grid(
  task_list,
  learner.list,
  kfoldcv)
```

Puisque notre but est d’expliquer la visualisation, nous avons déjà calculé et sauvegardé les résultats dans `library(animint2data)`.
Le code ci-dessous télécharge ces résultats.

```{r}
if(TRUE){
  if(!requireNamespace("animint2data"))
    remotes::install_github("animint/animint2data")
  data(bench.result, package="animint2data")
}else{ # to re-run execute the two lines below:
  if(require(future))plan("multisession")
  bench.result <- mlr3::benchmark(bench.grid, store_models = TRUE)
}
```

Si vous souhaitez recalculer ces résultats, vous pouvez exécuter la ligne avec `benchmark()` ci-dessus.
<!-- comment -->
Le calcul pourrait prendre plusieurs minutes ou même plusieurs heures, en fonction de votre ordinateur.
<!-- comment -->
Le code `plan("multisession")` fait que chaque combinaison (données, algorithmes, divisions) est calculée sur un différent processeur.
<!-- comment -->
Pour aller encore plus vite, vous pouvez utiliser une grappe de calcul [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Graphiques d’erreur et d’aire sous la courbe {#test-error-and-accuracy-plots}

<!-- comment -->

Nous commençons avec la définition des mesures à calculer pour évaluer les prédictions.

<!-- comment -->

```{r}
test_measure_list <- mlr3::msrs(c(
  'classif.auc','classif.ce','classif.tpr','classif.fpr'))
sapply(test_measure_list, function(M)M$label)
```

<!-- comment -->

La sortie ci-dessous affiche la correspondance entre les noms et les codes pour les mesures.

* `classif.auc` est l’aire sous la courbe ROC.
* `classif.ce` est le taux d’erreur de classification, soit le nombre d’étiquettes avec prédictions incorrectes, divisé par le nombre d’étiquettes total.
* `classif.tpr` est le taux de vrai positifs, soit le nombre d’étiquettes positives avec prédictions correctes, divisé par le nombre d’étiquettes positives.
* `classif.fpr` est le taux de faux positifs, soit le nombre d’étiquettes négatives avec prédictions incorrectes, divisé par le nombre d’étiquettes négatives.

<!-- comment -->
Ensuite, nous utilisons `$score()` pour calculer ces mesures sur les données test, dans chaque division de validation croisée.

```{r}
score_dt <- bench.result$score(test_measure_list)
score_dt[, .(task_id, learner_id, iteration, classif.auc, classif.ce)]
```

La sortie ci-dessus contient une ligne pour chaque combinaison de données, algorithme, division de validation croisée.
Il y a des colonnes pour deux mesures : l’AUC et le taux d’erreur.
<!-- comment -->
Le code ci-dessous visualise les taux d’erreur.
<!-- comment -->
Remarquons que nous convertissons le taux d’erreur en pourcentage afin d’economiser l’espace sur l’axe X (par exemple, 0.3 devient 30).

```{r}
library(animint2)
score_dt[, let(percent_error=100*classif.ce)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    percent_error, learner_id),
    data=score_dt)+
  scale_x_continuous(
    breaks=seq(0,100,by=10),
    limits=c(0,60))
```

Le graphique ci-dessus affiche trois cercles pour chaque Y — un cercle pour chaque division de validation croisée.
<!-- comment -->
Nous voyons que l’algorithme des plus proches voisins a le plus petit taux d’erreur dans certains jeux de données (`vowel`), et le réseau de neurones (`torch_dense_50`) est meilleur dans d’autres données (`spam`).
<!-- comment -->
Le but dans ce chapitre est de rajouter l’interactivité à ce graphique, pour montrer des détails pour chaque division de validation croisée.

<!-- comment -->

### Courbes ROC {#roc-curves}

<!-- comment -->

Dans cette section, nous affichons des courbe ROC, qui sont utile pour l’évaluation dans la classification binaire.
<!-- comment -->
En fait, `$score()` a déjà calculé l’aire sous la courbe (AUC), que nous allons visualiser avec le code ci-dessous.

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt)
```

Le graphique ci-dessus affiche l’AUC sur l’ensemble test, pour chaque division de validation croisée, chaque jeu de données, et chaque algorithme d’apprentissage.
<!-- comment -->
Les résultats d’AUC suggèrent que `zip` est le plus facile, et `sonar` est le plus dificile, ce qui est consistent avec les taux d’erreur affichés dans le graphique précédent.
<!-- comment -->
Comme attendu, `featureless` (l’algorithme qui prédit toujours la classe majoritaire) a toujours AUC=0.5, ce qui correspond à une fonction de prédiction qui est constante. 
<!-- comment -->
Le code ci-dessous enlève les résultats pour `featureless`, pour souligner les autres résultats.

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt[learner_id!="featureless"])
```

Nous ne voyons plus les résultats pour `featureless` dans le graphique ci-dessus.
<!-- comment -->
Le code ci-dessous refait le même graphique, avec quelques additions pour l’affichage interactif.
<!-- comment -->
Remarquons que la colonne `task_it` sera utile pour une variable de sélection (pour à la fois `task_id` et `iteration`).

```{r}
score_dt[, task_it := paste(task_id, iteration)]
(gg.auc <- ggplot()+
   theme_bw()+
   facet_grid(task_id ~ ., scales="free")+
   scale_x_continuous(
     "Test set Area Under the ROC Curve")+
   geom_point(aes(
     classif.auc*100, learner_id),
     clickSelects="task_it",
     size=5,
     fill="grey",
     color="black",
     color_off="grey",
     data=score_dt[learner_id!="featureless"]))
```

Ensuite, nous calculons une courbe ROC pour chaque combinaison de `task_id`, `learner_id`, et division de validation croisée (`iteration`).
<!-- comment -->
Remarquons que le vecteur d’étiquettes à prédire dans la classification binaire est un facteur avec deux niveaux.
<!-- comment -->
Dans `mlr3`, le premier niveau est la classe positive, alors nous utilisons la première colonne de `prob` pour calculer la courbe ROC.

```{r}
roc_dt <- score_dt[, {
  pred <- prediction_test[[1]]
  is.positive <- as.integer(pred$truth)==1
  label01 <- ifelse(is.positive, 1, 0)
  WeightedROC::WeightedROC(pred$prob[,1], label01)
}, by=.(task_id, learner_id, iteration, task_it)]
roc_dt[, .(task_it, learner_id, threshold, FPR, TPR)]
```

Le tableau ci-dessus représente les courbes ROC à utiliser pour évaluer les prédictions sur les ensembles test.
<!-- comment -->
Nous voyons que `FPR=TPR=1` quand `threshold=0`, et `FPR=TPR=0` quand `threshold=Inf`.
<!-- comment -->
Le code ci-dessous affiche les courbes ROC dans différents facettes.

```{r}
gg.roc <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected="task_it")+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc+facet_grid(iteration ~ task_id)
```

Nous voyons une facette pour chaque jeu de données, et pour chaque division de validation croisée.
<!-- comment -->
Ensuite, nous rajoutons un point pour montrer le seuil par défaut pour chaque algorithme.

```{r}
gg.roc.point <- gg.roc+
  geom_point(aes(
    classif.fpr, classif.tpr, fill=learner_id,
    tooltip=sprintf(
      "%s default FPR=%.3f TPR=%.3f, errors=%.1f%%\n",
      learner_id, classif.fpr, classif.tpr, classif.ce*100)),
    data=score_dt,
    size=4,
    color="black",
    showSelected="task_it")
gg.roc.point+facet_grid(iteration ~ task_id)
```

Ci-dessus nous voyons un point pour le seuil par défaut.
<!-- comment -->
Il est évident que les différents algorithmes ont différents valeurs de `FPR` et `TPR`, par défaut.
<!-- comment -->
Ensuite nous rajoutons texte pour afficher la sélection de données et division de validation croisée.

```{r}
score_dt[, lev_str := {
  pred <- prediction_test[[1]]
  paste(levels(pred$truth), collapse=" vs ")
}, by=task_id]
gg.roc.text <- gg.roc.point+
  geom_text(aes(
    0.5, 0,
    label=sprintf(
      "%s (%s) fold %d",
      task_id, lev_str, iteration)),
    data=score_dt[learner_id=="featureless"],
    showSelected="task_it")
gg.roc.text+facet_grid(iteration ~ task_id)
```

Le graphique affiche le texte en bas de chaque facette.
<!-- comment -->
Ensuite, nous calculons le rang de chaque algorithme dans chaque jeu de données et division.

```{r}
score_dt[
, auc.it.rank := rank(classif.auc)
, by=.(task_id, iteration, task_it)]
```

Les rangs sont utilisés dans l’affichage ci-dessous.
<!-- comment -->
Nous créons une visualisation interactive à partir des graphiques de ROC et AUC.

```{r viz-auc-roc}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split")+
    theme_animint(width=400, height=400),
  roc=gg.roc.text+
    ggtitle("ROC curves for selection")+
    theme_animint(width=400, height=400)+
    geom_label_aligned(aes(
      Inf, auc.it.rank*0.1, color=learner_id,
      label=sprintf("%s AUC=%.3f", learner_id, classif.auc)),
      data=score_dt,
      hjust=1,
      showSelected="task_it",
      alignment="vertical"))
```

Dans la visualisation, vous pouvez cliquer les points d’AUC en haut, ce qui affichera les courbes ROC correspondantes en bas.

<!-- comment -->

### Interaction avec courbes ROC {#interactive-roc-curves}

<!-- comment -->

Dans la section précédente, nous avons affiché un point sur chaque courbe ROC pour représenter le seuil par défaut.
<!-- comment -->
Ces points correspondent aux différents taux de faux positifs, pour les différents algorithmes d’apprentissage.
<!-- comment -->
Pour une comparaison plus juste entre les algorithmes, nous pouvons afficher le meilleur taux de vrais positifs, pour un seuil de faux positifs (à définir par la sélection interactive).

```{r}
(best_TPR_dt <- roc_dt[, .(
  best_TPR=max(TPR)
), by=.(task_id,iteration,task_it,learner_id,FPR)])
```

Le tableau ci-dessous contient le meilleur taux de vrai positifs (`best_TPR`), pour chaque seuil de faux positifs (`FPR`).
<!-- comment -->
Nous voulons seulement afficher les seuils pour lesquelles il y a tous les algorithmes, alors nous contons le nombre d’algorithmes pour chaque seuil.

```{r}
best_TPR_algos <- best_TPR_dt[, .(
  algos = .N
), by=.(task_id,iteration,task_it,FPR)]
table(best_TPR_algos$algos)
```

Dans le tableau ci-dessus, nous voyons que

* il y a 30 seuils extrêmes avec tous les 5 algorithmes (y compris `featureless`),
* il y a plusieurs seuils avec seulement entre 1 et 3 algorithmes, que nous ne voulons pas afficher.
<!-- comment -->
Nous voulons seulement garder les seuils avec au moins 4 algorithmes, ce que nous faisons avec le code ci-dessous.
<!-- comment -->
En plus, pour que le nombre de seuils reste abordable pour la sélection interactive, nous imposons un maximum de 40 seuils, avec espacement presque régulier à travers les valeurs possible pour le taux de faux positifs.

```{r}
(min.algos <- length(learner.list)-1)
max.FPR.values <- 40
(FPR_vlines <- best_TPR_algos[algos >= min.algos][
, keep := c(TRUE, diff(ceiling(FPR*max.FPR.values))!=0)
, by=.(task_id,iteration,task_it)][keep==TRUE])
```

Le tableau ci-dessus contient les seuils que nous pouvons sélectionner avec l’interactivité.
<!-- comment -->
Dans le code ci-dessous, nous dessinons chaque seuil avec une ligne verticale.

```{r}
gg.roc.fpr <- gg.roc.text+
  geom_vline(aes(
    xintercept=FPR),
    alpha=0.7,
    size=5,
    data=FPR_vlines,
    showSelected="task_it",
    clickSelects="FPR")
gg.roc.fpr+facet_grid(iteration ~ task_id)
```

Le graphique ci-dessus affiche toutes les seuils, ce qui fait un fond noir.
<!-- comment -->
Ensuite, nous faisons un nouveau graphique pour afficher les meilleurs taux de vrai positifs, pour le seuil sélectionné.

```{r}
best_TPR_show <- best_TPR_dt[
  FPR_vlines, on=.(task_id,iteration,task_it,FPR)]
(gg.tpr <- ggplot()+
   theme_bw()+
   theme(legend.position="none")+
   geom_point(aes(
     best_TPR, learner_id, color=learner_id),
     data=best_TPR_show,
     size=5,
     showSelected=c("task_it","FPR"))+
   geom_text(aes(
     best_TPR, learner_id, label=sprintf("%.4f", best_TPR)),
     data=best_TPR_show,
     showSelected=c("task_it","FPR")))
```

Le graphique ci-dessus affiche un point et du texte pour les meilleurs taux de vrai positifs.
<!-- comment -->
Ensuite nous combinons tous les graphiques précédants dans une visualisation interactive.

```{r viz-select-fpr}
(viz.select.fpr <- animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  roc=gg.roc.fpr+
    ggtitle("ROC curves for selection")+
    theme_animint(last_in_row=TRUE),
  TPR=gg.tpr+
    ggtitle("Best TPR for selected FPR")+
    theme_animint(colspan=2, width=800, height=200)))
```

La visualisation ci-dessus permet de

<!-- comment -->

* cliquer le graphique en haut à gauche pour choisir un jeux de données et une division de validation croisée (`task_it`),
<!-- comment -->
* cliquer le graphique en haut à droite pour choisir un seuil de taux de faux positifs (`FPR`),
<!-- comment -->
* ensuite de voir les meilleurs taux de vrais positifs, dans le graphique en bas.

<!-- comment -->

Si vous cliquez les seuils dans les données `sonar`, vous allez voir que le meilleur algorithme dépende du choix de seuil.
<!-- comment -->
Par exemple, `cv_glmnet` est le pire pour `FPR` petit, mais il est le meilleur pour `FPR` plus grand.

<!-- comment -->

### Rajouter un niveau de zoom {#adding-a-zoom-level}

<!-- comment -->

Les visualisations précédentes utilisent facettes pour afficher l’AUC pour toutet les différents jeux de données.
<!-- comment -->
Dans cette section, nous proposons d’afficher l’AUC pour seulement un jeu de données à la fois.
<!-- comment -->
Nous allons faire un graphique qui permet la sélection d’un jeu de données en fonction du nombre d’observations, et du max AUC, qu’on calcule avec le code ci-dessous.

```{r}
(summary_dt <- score_dt[, .(
  max_auc=max(classif.auc),
  nrow=task[[1]]$nrow
), by=task_id])
```

Le tableau ci-dessus contient une ligne pour chaque jeu de données.
<!-- comment -->
Nous utilisons ce tableau pour dessiner le nuage de points ci-dessous.

```{r}
point_size <- 5
point_fill <- "grey"
point_color <- "black"
point_color_off <- "grey"
(gg.summary <- ggplot()+
  theme_bw()+
  geom_point(aes(
    nrow, max_auc),
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    clickSelects="task_id",
    data=summary_dt))
```

Le graphique contient un point pour chaque jeu de données.
<!-- comment -->
Remarquons dans le code ci-dessus que nous avons `clickSelects="task_id"`, ce qui permet la sélection d’un jeu de données seulement (pas la division de validation croisée).
<!-- comment -->
Ensuite nous créons un graphique avec `showSelected="task_id"` et `clickSelects="iteration"`.

```{r}
gg.auc.show.task <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt)
gg.auc.show.task+facet_grid(task_id ~ .)
```

Le graphique contient un panneau pour chaque jeu de données.
<!-- comment -->
Remarquons que les valeurs d’AUC sont très différent entre les jeux de données,et que `featureless` a toujours AUC=0.5, comme attendu.
<!-- comment -->
Nous supprimons `featureless` dans le graphique ci-dessous.

```{r}
gg.auc.show.task.zoom <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt[learner_id != "featureless"])
gg.auc.show.task.zoom+facet_grid(task_id ~ .)
```

Le graphique ci-dessus ne contient plus `featureless`.
<!-- comment -->
Ensuite nous faisons un graphique des courbes ROC pour la sélection de `task_id` et `iteration`.

```{r}
gg.roc.show.task.it <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected=c("task_id","iteration"))+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc.show.task.it+facet_grid(iteration ~ task_id)
```

Le graphique affiche un panneau pour chaque jeu de données, et pour chaque division de validation croisée.
<!-- comment -->
Le code ci-dessous combine ces graphiques dans une visualisation interactive.

```{r viz-two-selectors}
animint(
  summary=gg.summary+
    ggtitle("1. Select task"),
  roc=gg.roc.show.task.it+
    ggtitle("ROC curves for selected task and iteration")+
    theme_animint(last_in_row=TRUE),
  testAUC=gg.auc.show.task+
    ggtitle("2. Select iteration, all learners")+
    theme_animint(
      width=800, height=200, last_in_row=TRUE, colspan=2),
  testAUCzoom=gg.auc.show.task.zoom+
    ggtitle("2. Select iteration, zoom to non-trivial")+
    theme_animint(
      update_axes="x",
      width=800, height=200, last_in_row=TRUE, colspan=2))
```

Le graphique affiche

<!-- comment -->

* en haut à gauche, un résumé de tous les jeux de données (cliquez un point pour sélectionner un jeu de données).
<!-- comment -->
* en bas, deux graphiques d’AUC sur les ensembles test, pour le jeu de données sélectionné (cliquez un point pour sélectionner une division de validation croisée).
<!-- comment -->
* en haut à droite, les courbes ROC pour la sélection de `task_id` et `iteration`.

<!-- comment -->

### Tests statistiques pour différences entre algorithmes {#p-values-learners}

<!-- comment -->

Souvent nous voulons savoir s’il existe des différences significatives entre l’AUC de deux algorithmes (les deux avec AUC le plus grand).
Dans cette section, nous calculons des valeurs-p pour évaluer combien les différences sont significatives.
<!-- comment -->
Nous commençons avec le calcul de moyenne et écart type, sur les divisions de validation croisée.

```{r}
(score_stats <- dcast(
  score_dt[learner_id!="featureless"],
  task_id + learner_id ~ .,
  list(mean, sd),
  value.var="classif.auc"))
```

Nous visualisons ces statistiques avec le code ci-dessous.

```{r}
score_stats[, let(
  lo=classif.auc_mean-classif.auc_sd,
  hi=classif.auc_mean+classif.auc_sd)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc_mean, learner_id),
    data=score_stats)+
  geom_segment(aes(
    hi, learner_id,
    xend=lo, yend=learner_id),
    data=score_stats)+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)",
    breaks=seq(0,1,by=0.1))
```

Ci-dessus nous voyons un point pour chaque moyenne, et un segment pour l’écart type.
<!-- comment -->
Ensuite nous calculons le rang de chaque algorithme, pour chaque jeu de données.

```{r}
score_stats[, auc.task.rank := rank(-classif.auc_mean), by=task_id][]
```

Ci-dessus nous voyons une nouelle colonne `auc.task.rank` avec valeurs entre 1 et 4.
<!-- comment -->
Nous utilisons ces valeurs sur l’axe Y dans le graphique ci-dessous.

```{r}
gg.rank <- ggplot()+
  geom_point(aes(
    classif.auc_mean, auc.task.rank),
    data=score_stats)+
  geom_segment(aes(
    classif.auc_mean+classif.auc_sd, auc.task.rank,
    xend=classif.auc_mean-classif.auc_sd, yend=auc.task.rank),
    data=score_stats)+
  scale_y_continuous(
    "Algorithm rank using test AUC",
    breaks=1:4,
    limits=c(1,5))+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)")
gg.rank+facet_wrap("task_id", scales="free")
```

Ci-dessus nous voyons les rangs, mais nous ne voyons pas les noms.
<!-- comment -->
Nous rajoutons les noms ci-dessous.

```{r}
gg.rank.text <- gg.rank+
  geom_text(aes(
    -Inf, auc.task.rank,
    label=sprintf(
      "%s %.4f±%.4f",
      learner_id,
      classif.auc_mean, classif.auc_sd)),
    hjust=0, vjust=-0.5, size=10,
    data=score_stats)
gg.rank.text+facet_wrap("task_id", scales="free")
```

Pour tester les différences entre les rangs adjacents, nous faisons une jointure entre chaque rang (`auc.task.rank`) et le rang suivant (`next.rank`).

```{r}
score_dt_rank <- score_stats[, .(auc.task.rank, learner_id, task_id)][
  score_dt, on=.(learner_id, task_id), nomatch=0L]
(join_dt <- score_dt_rank[, next.rank := auc.task.rank+1][
  score_dt_rank,
  .(task_id, iteration,
    bad.rank=x.auc.task.rank,
    good.rank=i.auc.task.rank,
    bad.auc=x.classif.auc,
    good.auc=i.classif.auc),
  on=.(task_id, iteration, auc.task.rank=next.rank),
  nomatch=0L])
```

Ensuite nous faisons des tests de Student.

```{r}
(pval_dt <- join_dt[, .SDcols=c("good.auc","bad.auc"), {
  mean_dt <- lapply(.SD, mean)
  paired <- t.test(
    bad.auc, good.auc, alternative = "l", paired=TRUE)
  c(mean_dt, p.paired=paired$p.value)
}, keyby=.(task_id, good.rank, bad.rank)])
```

TODO Above we see a table with one row per T-test result.
<!-- comment -->
We plot the differences between means in red below.

```{r}
gg.rank.seg <- gg.rank+
  geom_segment(aes(
    mean.lo, auc.task.rank+0.5,
    xend=mean.hi, yend=auc.task.rank+0.5),
    color="red",
    data=pval_dt)
gg.rank.seg+facet_wrap("task_id", scales="free")
```

Above we see red segments highlighting differences between adjacent means.
<!-- comment -->
Below we add text to show P-values.

```{r}
gg.rank.p <- gg.rank.seg+
  geom_text(aes(
    mean.lo, auc.task.rank+0.5, label=ifelse(
      p.paired<0.01, "P<0.01",
      sprintf("P=%.2f", p.paired))),
    color="red", hjust=1, size=10,
    data=pval_dt)
gg.rank.p+facet_wrap("task_id", scales="free")
```

Above we see P-values have been added.

<!-- comment -->

* One P-value is less than 0.01, indicating a difference that is statistically significant, between ranks 2 and 3 in `sonar`.
<!-- comment -->
* Most P-values are greater than 0.05, indicating that these differences are not statistically significant.
<!-- comment -->
* Because of the small sample size (3 train/test splits), increasing the number of cross-validation folds may decrease some of these P-values (exercise for the reader).
<!-- comment -->
* Exercise: combine P-values with algorithm names and mean/SD on the same plot. Use this plot instead of `testAUCzoom` in the previous animint.

<!-- comment -->

## Checking for over- and under-fitting {#over-under-fitting}

<!-- comment -->

In this section, our goal is to make various figures that check if each algorithm is regularized appropriately.
<!-- comment -->
Each machine learning algorithm has a different hyper-parameter that is used to avoid under- and over-fitting.

<!-- comment -->

* nearest neighbors has the number of neighbors: small number of neighbors may overfit, large number of neighbors may underfit.
<!-- comment -->
* `torch` learners has the number of gradient descent epochs: large number of epochs may overfit, small number of epochs may underfit.
<!-- comment -->
* the `glmnet` L1-regularized linear model has the penalty `lambda`: small penalty may overfit, large penalty may underfit.

<!-- comment -->

### Nearest neighbors {#nearest-neighbors-selection}

<!-- comment -->

In this section, we show how to verify is the number of neighbors was selected appropriately, to avoid over- and under-fitting.
<!-- comment -->
First, we compute a table with one row per train/test split, and columns which contain information on how many neighbors were used for prediction.

```{r}
(best_dt <- score_dt[grep("knn", learner_id), {
  arch <- learner[[1]]$archive
  adata <- arch$data[, status := ifelse(
    classif.auc==max(classif.auc), "max", "other")]
  arch$best()[, .(k, details=list(adata))]
}, by=.(task_id, iteration, task_it)])
```

The table above has columns

<!-- comment -->

* `k`, the number of neighbors selected as best.
<!-- comment -->
* `details`, a table of information about how that number of neighbors was selected.

<!-- comment -->

Below, we create a new table by combining all of the details tables.

```{r}
details_dt <- best_dt[
, details[[1]][order(k)]
, by=.(task_id, iteration, task_it)]
details_dt[, .(task_id, iteration, k, classif.auc, status)]
```

The table above shows details about what was the validation AUC for each potential value of `k`, the number of neighbors.
<!-- comment -->
Below we plot the validation AUC as a function of the number of neighbors, emphasizing the selected value and the max.

```{r}
gg.neighbors <- ggplot()+
  geom_vline(aes(
    xintercept=k),
    showSelected="task_it",
    data=best_dt)+
  geom_point(aes(
    k, classif.auc, color=status),
    showSelected="task_it",
    data=details_dt)+
  scale_y_continuous("Validation set AUC")
gg.neighbors+facet_grid(task_id ~ iteration)
```

Above we see how the AUC was selected for each task and iteration.

<!-- comment -->

* Some data sets (`sonar`, `spam`, `vowel`) have a clear max at an intermediate number of neighbors, which avoids both over- and under-fitting. For `vowel` iteration 2, max AUC occurs at the extreme value of 1 neighbor, which is unusual, but does not suggest underfitting in this case, because there is no smaller value of neighbors than 1.
<!-- comment -->
* Some data sets (`waveform`, `zip`) have max AUC at the largest number of neighbors (40), which suggests overfitting. These data suggest that an even larger AUC may be obtained by increasing the number of neighbors past 40, until we see a clear decrease of the AUC values.

<!-- comment -->

Below we make an interactive version of the same plot.

```{r viz-neighbors}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  knnDetails=gg.neighbors+
    ggtitle("Nearest neighbors model selection")+
    theme_animint(update_axes="y", last_in_row=TRUE))
```

Above we can click on the left plot to select a data set and cross-validation fold.
<!-- comment -->
Clicking on `zip` or `waveform`, the data sets with largest test AUC, shows that max validation AUC occurs for large numbers of neighbors.

<!-- comment -->

### Torch {#torch}

<!-- comment -->

In this section, we explore early stopping regularization with `torch` learners.
<!-- comment -->
First, we compute the selected number of epochs.

```{r}
score_torch <- score_dt[
  grepl("torch",learner_id)
][, best_epoch := sapply(
  learner, function(L)unlist(L$tuning_result$internal_tuned_values)
)]
```

Below, we extract the model training history.

```{r}
(history_torch <- score_torch[, {
  L <- learner[[1]]
  M <- L$archive$learners(1)[[1]]$model
  M$torch_model_classif$model$callbacks$history
}, by=.(task_id, learner_id, iteration, task_it)])
```

The table above has one row for each epoch, and columns

<!-- comment -->

* `train.classif.logloss`, logistic (cross-entropy) loss on the set used to compute gradients, which always should decrease, if learning rate and batch size have been chosen appropriately. Note that “train” is a potentially confusing name for this set, so for increased clarity, we rename it to “subtrain” below.
<!-- comment -->
* `valid.classif.logloss`, logistic (cross-entropy) loss on the held-out set used to regularize (not used to compute gradients), which was minimized to select the best regularization (here, the number of epochs). Similarly, `valid.classif.auc` could have been maximized, if `classif.auc` was the first item in `measure_list`.

<!-- comment -->

Below, we reshape and rename the previous history data.

```{r}
(history_long <- nc::capture_melt_single(
  history_torch,
  set=nc::alevels(valid="validation", train="subtrain"),
  ".classif.",
  measure=nc::alevels("logloss", auc="AUC")))
```

The table above has new columns `set`, `measure`, and `value`.
<!-- comment -->
In the code below, we plot these data for one data set and cross-validation iteration.

```{r}
one_split <- function(DT,it=1)DT[iteration==it & task_id=="sonar"]
one_split_history <- one_split(history_long)
(gg.torch.line <- ggplot()+
  theme_bw()+
  facet_grid(measure ~ learner_id, labeller=label_both, scales="free")+
  geom_line(aes(
    epoch, value, color=set),
    data=one_split_history))
```

The figure above shows learning curves that are typical of early stopping regularization using torch models.

<!-- comment -->

* log loss always decreases, and AUC always increases, for the subtrain set (used to compute gradients).
<!-- comment -->
* The typical U shape is clearly visible for the validation log loss. Typically we select the number of epochs which minimizes this curve (or maximizes the validation AUC).

<!-- comment -->

Below we add points which emphasize the min log loss, and max AUC, of each curve.

```{r}
history_best <- history_long[, {
  mfun <- if(measure=="AUC")max else min
  .SD[value==mfun(value)]
}, by=.(task_id, iteration, task_it, learner_id, measure, set)
][, point := "best"]
one_split_best <- one_split(history_best)
(gg.torch.point <- gg.torch.line+
  geom_point(aes(
    epoch, value, color=set, fill=point),
    data=one_split_best)+
   scale_fill_manual(values=c(best="black")))
```

Above we can see that

<!-- comment -->

* there is only one min for each log loss curve.
<!-- comment -->
* there are sometimes more than one max for the AUC curves.
<!-- comment -->
* the subtrain AUC quickly maxes out at 1, whereas the log loss continues decreasing.

<!-- comment -->

Below we add vertical lines and text to emphasize the number of epochs which was selected during training.

```{r}
one_split_score <- one_split(score_torch)
(gg.torch.text <- gg.torch.point+
  geom_vline(aes(
    xintercept=best_epoch),
    data=one_split_score)+
  geom_text(aes(
    best_epoch, -Inf, label=paste0(" selected epoch=", best_epoch)),
    vjust=-0.5, hjust=0,
    data=one_split_score))
```

Above we can see that the selected number of epochs is consistent with the min validation log loss, which happens before the max validation AUC.
<!-- comment -->
Next, we propose an overview, which displays the best epochs for each data set, learning algorithm, and cross-validation iteration.

```{r}
(history_best_wide <- dcast(
  history_best,
  learner_id + set + task_id + iteration + task_it ~ measure,
  list(min, max, mid=function(x)(min(x)+max(x))/2),
  value.var="epoch"))
```

The table above has additional columns for min, max, and mid number of epochs which are best (according to validation log loss or AUC).
<!-- comment -->
We use the mid values as points in the overview plot below.

```{r}
history_best_valid <- history_best_wide[set=="validation"]
learner.colors <- c(
  torch_linear="grey",
  torch_dense_50="red")
(gg.best.valid <- ggplot()+
   theme_bw()+
   scale_fill_manual(values=learner.colors)+
   xlab("Epochs with best validation log loss")+
   ylab("Epochs with best validation AUC")+
   geom_point(aes(
     epoch_mid_logloss, epoch_mid_AUC, fill=learner_id),
     clickSelects="task_it", size=5,
     color_off="white", color="black",
     data=history_best_valid)+
   facet_wrap("task_id"))
```

The plot above shows that 

<!-- comment -->

* some data sets (`sonar` and `waveform`) have best validation loss and AUC at an intermediate number of epochs. This is evidence of a good fit (neither under- nor over-fitting).
<!-- comment -->
* in some data sets (`vowel` and `zip`), the best validation log loss occurs at 200 epochs. This is evidence of underfitting (number of epochs could be increased to find better validation loss values).

<!-- comment -->

Below we add a segment to show the range of epochs which attain the best validation AUC.

```{r}
(gg.best.seg <- gg.best.valid+
   scale_color_manual(values=learner.colors)+
   geom_segment(aes(
     epoch_mid_logloss, epoch_min_AUC,
     xend=epoch_mid_logloss, yend=epoch_max_AUC,
     color=learner_id),
     size=3,
     clickSelects="task_it",
     data=history_best_valid))
```

Above we see that there are some runs in `vowel` and `zip` for which there is a range of epochs that achieve the best validation AUC.
<!-- comment -->
Next, we prepare a plot of learning curve details, by first computing relative values between 0 and 1 (since values can be very different between data sets).

```{r}
norm01 <- function(x)(x-min(x))/(max(x)-min(x))
history_long[
, relative_values := norm01(value)
, by=.(iteration, task_id, learner_id, set, measure)]
gg.log.auc <- ggplot()+
   theme_bw()+
   facet_grid(measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   geom_line(aes(
     epoch, relative_values, color=set, group=set),
     showSelected="task_it",
     data=history_long)
```

Finally, we combine the two previous plots into an interactive visualization using the code below.

```{r viz-torch}
(viz.torch <- animint(
  overview=gg.best.seg+
    theme_animint(width=800, colspan=2, last_in_row=TRUE)+
    ggtitle("Select task and cross-validation iteration"),
  details=gg.log.auc+
    ggtitle("Selected learning curves")))
```

Above we can see a data visualization with two plots:

<!-- comment -->
* Clicking the top overview plot selects a data set and cross-validation iteration.
<!-- comment -->
* The bottom plot shows the learning curves for the current selection.

<!-- comment -->

For some data sets, the AUC quickly attains a max near 1, and it is difficult to see how close it is to 1.
<!-- comment -->
To more easily visualize those details, the code below computes inverse AUC (1-AUC).
<!-- comment -->
The best value for AUC is 1, whereas it is 0 for inverse AUC.
<!-- comment -->
So using inverse AUC with a log scale, as in the code below, will show more details of how close it gets to 0.

```{r}
history_long[, let(
  Measure = factor(
    ifelse(measure=="logloss", "logloss", "InvAUC"),
    c("logloss","InvAUC")),
  Relative_values = ifelse(
    measure=="logloss", relative_values, 1-relative_values))]
gg.log.scale <- ggplot()+
   theme_bw()+
   facet_grid(Measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   scale_y_log10()+
   geom_line(aes(
     epoch, Relative_values, color=set, group=set),
     showSelected=c("task_it", "set"),
     data=history_long)
```

Note in the code above that we use capital `Measure` and `Relative_values`, along with a log-scale Y axis.
<!-- comment -->
The code below adds this new log-scale plot to the previous data visualization.

```{r viz-torch-log}
viz.torch.log <- viz.torch
viz.torch.log$detailsLog <- gg.log.scale+
  ggtitle("Selected learning curves (log scale)")+
  theme(legend.position="none")
viz.torch.log
```

The visualization above has an additional plot on the bottom right, which shows details for the selected data set and cross-validation iteration.
<!-- comment -->
For an example of additional details shown using inverse AUC, try selecting one of the cross-validation iterations in the `vowel` data set.

### glmnet {#glmnet}

In this section, we explore visualizations of the `glmnet` L1-regularized linear model.
<!-- comment -->
We begin by using the `plot()` method of the model stored in the learner object.

```{r}
score_glmnet <- score_dt[grep("glmnet",learner_id)]
L <- score_glmnet$learner[[1]]
library(glmnet)
plot(L$model)
```

The plot above shows the validation set Binomial Deviance (same as log loss in previous section) as a function of the model complexity (measured by negative log penalty).
<!-- comment -->
We see the curve decrease, but not increase, which may indicate underfitting.
<!-- comment -->
To reproduce this plot, we use the ggplot code below.

```{r}
cv_glmnet_one <- with(L$model, data.table(nzero, lambda, cvm, cvsd))
(gg.glmnet <- ggplot()+
   scale_y_continuous("Validation log loss")+
   geom_segment(aes(
     -log(lambda), cvm+cvsd,
     xend=-log(lambda), yend=cvm-cvsd),
     data=cv_glmnet_one)+
   geom_point(aes(
     -log(lambda), cvm, fill=nzero),
     data=cv_glmnet_one)+
   scale_fill_gradient(low="white", high="red"))
```

Above we see a ggplot display which is similar to the previous plot.
<!-- comment -->
Next, we add a label to show the number of active variables at various regularization parameters.

```{r}
gg.glmnet+
  geom_text(aes(
    -log(lambda), Inf, label=nzero),
    vjust=1,
    data=cv_glmnet_one[seq(1,.N,by=5)])
```

The plot above has text labels at the top which show the number of non-zero weights for several regularization parameters.
<!-- comment -->
The goal will be to make a similar interactive display.
<!-- comment -->
We compute a larger data table of results using the code below.

```{r}
(cv_glmnet_all <- score_glmnet[, with(learner[[1]]$model, data.table(
  nzero, lambda, cvm, cvsd
)), by=.(task_id, iteration, task_it)])
```

The table above contains one row per data set, cross-validation iteration, and regularization parameter.
<!-- comment -->
Below we display the validation log loss for each iteration as a different line, with each data set as a different panel.

```{r}
ggplot()+
  scale_y_continuous("Validation log loss")+
  geom_line(aes(
    -log(lambda), cvm, group=iteration),
    data=cv_glmnet_all)+
  facet_wrap("task_id", scales="free")
```

Above we can see that the linear model can overfit if not properly regularized in some data sets (`sonar`, `vowel`, `waveform`), whereas there is evidence of underfitting in other data sets (`spam` and `zip`).

<!-- comment -->

## Chapter summary and exercises {#ch20-exercises}

<!-- comment -->

We have created several data visualizations related to `mlr3` benchmarks.

<!-- comment -->

Exercises:

<!-- comment -->

* In `viz.select.fpr`, there are no smooth transitions. Add a smooth transition for the `FPR` variable, and make sure to set `aes(key)` on the `geom_point()` and `geom_text()` in `gg.tpr`.
<!-- comment -->
* In `viz.select.fpr`, a separate plot is used to display Best TPR for selected FPR. Remove that plot, and add a `geom_point()` and `geom_label_aligned()` to the ROC curve plot, which show the same information.
<!-- comment -->
* In `gg.summary`, currently only the max AUC is shown. Add `geom_segment()` that shows the variation in AUC among algorithms.
<!-- comment -->
* In `viz.select.fpr` we showed Best TPR for selected FPR.
<!-- comment -->
  * Add a plot “Best error rate for selected FPR”. Hint: you will need to join the number of labels to the ROC curve table.
<!-- comment -->
  * Add `geom_text()` which shows the selected value of FPR.
<!-- comment -->
  * Make a re-design that shows best FPR for selected TPR.
<!-- comment -->
* Change the color legends so that the two linear models (`cv_glmnet` and `torch_linear`) have different shades of the same color (for example dark and light purple).
<!-- comment -->
* In visualizations with ROC curves, add `aes(color)` or `aes(fill)` to the other plots, with a consistent color scheme that matches the ROC curves. Make sure there is only one legend (with entries ordered to match the order of Y axes that display `learner_id`), and that it can make all data for a learner disappear, across all plots.
<!-- comment -->
* In `viz.torch.log`, add points and lines to emphasize the best and selected epochs.
<!-- comment -->
* Each section of this chapter has presented a different data visualization technique. Combine several different techniques in a single data visualization:
<!-- comment -->
  * Show model selection plots for all three algorithms (`glmnet`, `torch`, nearest neighbors).
<!-- comment -->
  * Show interactive ROC curves at the same time.

<!-- comment -->

Next, [the appendix](/ch99) explains some R programming idioms that are generally useful for interactive data visualization design.
