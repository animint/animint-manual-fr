# Apprentissage artificiel avec `mlr3`

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="ch20-figures/")
if(FALSE){
  knitr::knit("index.qmd")
}
```

Dans ce chapitre, nous explorerons plusieurs visualisations pour comparer la performance des algorithmes d’apprentissage artificiel; nous utiliserons des packages `mlr3`.

<!-- comment -->

* Nous démontrons d'abord la pertinence des packages `mlr3` pour la comparaison d'algorithmes d’apprentissage artificiel.
<!-- comment -->
* Nous utilisons ensuite `mlr3` pour définir et exécuter une évaluation comparative de plusieurs problèmes de classification binaire, et de plusieurs algorithmes d’apprentissage (plus proches voisins, modèles linéaires, réseaux de neurones).
<!-- comment -->
* Nous générons différents graphiques de taux d’erreur et d’aire sous la courbe ROC.
<!-- comment -->
* Nous ajoutons l’interactivité aux courbes ROC, pour présenter plusieurs niveaux de détail (données, itération de validation croisée, seuil des faux positifs).
<!-- comment -->
* Nous visualisons les mesures de fidélité et d’erreur par rapport à l’ensemble de validation, pour confirmer que le surapprentissage et le sous-apprentissage ont été évités.

<!-- comment -->

## Avantages de `mlr3` {#avantages-mlr3}

<!-- comment -->

Plusieurs packages R proposent des algorithmes d’apprentissage artificiel, par exemple :

<!-- comment -->

* `library(kknn)` offre des K Plus Proches Voisins (KPPV), présentés dans le [chapitre 10](/ch10).
<!-- comment -->
* `library(glmnet)` offre des modèles linéaires avec régularisation L1, comme le LASSO, que nous avons explorés dans le [chapitre 11](/ch11).
<!-- comment -->
* `library(torch)` offre des réseaux des neurones, exposés dans le [chapitre 18](/ch18).

<!-- comment -->

Pour obtenir de bons résultats de prédiction sur un jeu de données, nous entraînons plusieurs algorithmes, car nous ignorons quel algorithme aura les meilleures performances pour ces données.
<!-- comment -->
Nous utilisons donc différents packages et analysons ensuite les résultats pour déterminer quel algorithme fournit les meilleures prédictions.
<!-- comment -->
Dans chaque package, les fonctions pour l’apprentissage et la prédiction ont différents noms, et différentes interfaces.
<!-- comment -->
En général, chaque package contient une fonction `predict()` qui calcule les prédictions, mais le type de sortie est différent d'un package à l'autre (`factor` pour classe, `matrix` pour probabilités de chaque classe, etc.).
<!-- comment -->
Comment adapter un jeux de données à chaque package d’apprentissage ?
<!-- comment -->
Comment uniformiser les types de sortie pour permettre la comparaison de résultats ?
<!-- comment -->
Le package `mlr3` fournit plusieurs fonctions qui simplifient ce type d’analyse, appelé « benchmark » [@bischl2024applied].

<!-- comment -->

* Chaque jeu de données est représenté comme un `Task`, c'est-à-dire un tableau de données d’apprentissage accompagné de quelques métadonnées indiquant les colonnes à utiliser pour différents rôles (entrée, sortie, etc.).
<!-- comment -->
* Chaque algorithme d’apprentissage est représenté comme une classe  `Learner` qui fournit les prédictions dans un type de sortie uniforme (une matrice de probabilités pour chaque classe).

<!-- comment -->

Dans ce chapitre, nous expliquons l’utilisation d’`animint2` pour la visualisation interactive des résultats des différents algorithmes d’apprentissage sur différents jeux de données.
<!-- comment -->
Le code utilisé dans ce chapitre est une version modifiée d’un code pour la création des visualisations statiques [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Définition et exécution d'une analyse comparative {#exécution-comparaison}

<!-- comment -->

Dans `mlr3`, une comparaison « benchmark » comprend le calcul de toutes les combinaisons des éléments suivants :

<!-- comment -->

* une liste de jeux de données (`Tasks`),
<!-- comment -->
* une liste d’algorithmes d’apprentissage (`Learners`),
<!-- comment -->
* et toutes les itérations d’une méthode d’échantillonage (`Resampling`).

<!-- comment -->

### Les jeux de données (`Tasks`) {#tasks}

<!-- comment -->

Nous commençons avec une liste de deux jeux de données.

```{r}
mlr.tasks <- c("spam","sonar")
task_list <- mlr3::tsks(mlr.tasks)
```

Ensuite, nous définissons une fonction pour télécharger un jeu de données depuis le site web du livre « Elements of Statistical Learning » [@Hastie2009].

```{r}
library(data.table)
prefix <- "https://hastie.su.domains/ElemStatLearn/datasets/"
cache.fread <- function(data.name, f){
  cache.dir <- file.path("data", data.name)
  dir.create(cache.dir, showWarnings=FALSE, recursive=TRUE)
  local.path <- file.path(cache.dir, f)
  if(!file.exists(local.path)){
    u <- paste0(prefix, f)
    download.file(u, local.path)
  }
  fread(local.path)
}
```

Ensuite, nous faisons un boucle pour télécharger trois jeux de données.
Pour chacun, on rajoute un `Task` de classification binaire (avec seulement les deux premières classes).

```r
data.sets <- c("vowel","waveform","zip")
for(data.name in data.sets){
  suffix <- if(data.name=="zip")".gz" else ""
  set.list <- list()
  for(predefined.set in c("test","train")){
    one.set.dt <- cache.fread(
      data.name,
      paste0(data.name, ".", predefined.set, suffix)
    )
    if("row.names" %in% names(one.set.dt)){
      one.set.dt[, row.names := NULL]
    }
    setnames(one.set.dt, old=names(one.set.dt)[1], new="y")
    set.list[[predefined.set]] <- one.set.dt
  }
  task.dt <- rbindlist(set.list)[
    y %in% unique(y)[1:2] #only first two classes.
  ][, y := factor(y)]
  one.task <- mlr3::TaskClassif$new(data.name, task.dt, target='y')
  task_list[[data.name]] <- one.task
}
task_list
```

Le résultat ci-dessus est une liste représentant 5 problèmes de classification binaire :

<!-- comment -->

* `spam` représente la classification des courriels (`spam` est pourriel non-désiré, `nonspam` est courriel désiré) à partir d’un vecteur de comtages de mots « bag of words ».
<!-- comment -->
* [`sonar`](https://rdrr.io/cran/mlbench/man/Sonar.html) représente la classification de métal et roche, à partir d’un vecteur de bandes de fréquences sonores.
<!-- comment -->
* `vowel` représente la classification des sons voyelles, à partir de dix variables calculées depuis un fichier audio.
<!-- comment -->
* `waveform` est un jeu de données de simulation [@Hastie2009].
<!-- comment -->
* `zip` représente la classification de chiffres écrits à la main, à partir d’une image de 16x16 pixels, en échelle de gris.

<!-- comment -->

### Les algorithmes d’apprentissage {#learners}

<!-- comment -->

Dans cette section, nous définissons les algorithmes d’apprentissage à utiliser avec chaque jeu de données.
<!-- comment -->
Le code ci-dessous commence avec une liste de deux algorithmes :

<!-- comment -->

* `cv_glmnet` est un modèle linaire, avec régularisation L1 qui minimise le taux d’erreur dans la validation croisée à 10 divisions.
<!-- comment -->
* `featureless` est le modèle qui n’utilise pas les variables d’entrée, et qui va toujours prédire la classe la plus fréquente dans les données d’apprentissage.

<!-- comment -->

```{r}
learner.list <- list(
  mlr3learners::LearnerClassifCVGlmnet$new()$configure(id="cv_glmnet"),
  mlr3::LearnerClassifFeatureless$new()$configure(id="featureless"))
```

Ensuite, nous rajoutons l’algorithme des plus proches voisins.

```{r}
knn_learner <- mlr3learners::LearnerClassifKKNN$new()
knn_learner$param_set$values$k <- paradox::to_tune(1, 40)
```

Le code ci-dessus précise les bornes pour `k`, le nombre de voisins, entre 1 et 40.
<!-- comment -->
Le code ci-dessous précise la méthode pour la séléction de `k`, soit recherche avec 10 valeurs, avec validation croisée à 3 divisions, maximisant l’aire sous la courbe ROC, soit en anglais « Area Under the Curve » ou « AUC ».
<!-- comment -->
Rémarquons que si on veut calculer l’AUC pour l’ensemble de validation, il faut préciser `predict_type="prob"`, ce qui fait que la prédiction est un nombre réel (et non seulement la classe la plus probable).

```{r}
kfoldcv <- mlr3::rsmp("cv")
kfoldcv$param_set$values$folds <- 3
knn_learner$predict_type <- "prob"
learner.list$knn <- mlr3tuning::auto_tuner(
  learner = knn_learner,
  tuner = mlr3tuning::tnr("grid_search"),
  resampling = kfoldcv,
  measure = mlr3::msr("classif.auc", minimize = FALSE))
```

Ensuite, nous rajoutons deux algorithmes qui utilise `library(torch)`.
<!-- comment -->
On utilise une fonction qui renvoye une liste d’opérateurs qui sera utilisé pour définir l’algorithme.

```{r}
measure_list <- mlr3::msrs(c("classif.logloss", "classif.auc"))
n.epochs <- 200
make_po_list <- function(...)list(
  mlr3pipelines::po("scale"),
  mlr3pipelines::po(
    "select",
    selector = mlr3pipelines::selector_type(c("numeric", "integer"))),
  mlr3torch::PipeOpTorchIngressNumeric$new(),
  ...,
  mlr3pipelines::po("nn_head"),
  mlr3pipelines::po(
    "torch_loss",
    mlr3torch::t_loss("cross_entropy")),
  mlr3pipelines::po(
    "torch_optimizer",
    mlr3torch::t_opt("sgd", lr=0.1)),
  mlr3pipelines::po(
    "torch_callbacks",
    mlr3torch::t_clbk("history")),
  mlr3pipelines::po(
    "torch_model_classif",
    batch_size = 10,
    patience=n.epochs,
    measures_valid=measure_list,
    measures_train=measure_list,
    predict_type="prob",
    epochs = paradox::to_tune(upper = n.epochs, internal = TRUE)))
```

Remarquons dans le code ci-dessus que `lr` est le taux d’apprentissage, ou « learning rate » en anglais.
<!-- comment -->
La fonction ci-dessous définit que

<!-- comment -->

* la moitié des données d’apprentissage comme ensemble de validation (`validate = 0.5`), et l’autre moitié est l’ensemble sous-entraînement (pour calculer les gradients).
<!-- comment -->
* le nombre d’époques est choisi en minimisant la première mesure dans la liste `measure_list` (perte logistique).

```{r}
make_torch_learner <- function(id,...){
  po_list <- make_po_list(...)
  graph <- Reduce(mlr3pipelines::concat_graphs, po_list)
  glearner <- mlr3::as_learner(graph)
  mlr3::set_validate(glearner, validate = 0.5)
  mlr3tuning::auto_tuner(
    learner = glearner,
    tuner = mlr3tuning::tnr("internal"),
    resampling = mlr3::rsmp("insample"),
    measure = mlr3::msr("internal_valid_score", minimize = TRUE),
    term_evals = 1, id = id, store_models = TRUE)
}
```

Le code ci-dessous rajoute deux algorithmes d’apprentissage.

```{r}
if(torch::torch_is_installed()){
  learner.list$linear <- make_torch_learner("torch_linear")
  learner.list$dense <- make_torch_learner(
    "torch_dense_50",
    mlr3pipelines::po(
      "nn_linear",
      out_features = 50),
    mlr3pipelines::po("nn_relu_1", inplace = TRUE))
}
```

### Calculer les résultats {#running-the-benchmark}

<!-- comment -->

Avant de calculer les résultats pour des algorithmes de classification, on fait `predict_type=prob` pour s’assurer que les prédictions soient des valeurs réelles (pour pouvoir calculer les mesures d’évaluation comme l’AUC).

```{r}
for(learner.i in seq_along(learner.list)){
  learner.list[[learner.i]]$predict_type <- "prob"
}
```

Ensuite, nous combinons les jeux de données, les algorithmes d’apprentissage, et la méthode d’échantillonage (validation croisée à 3 divisions).

```{r}
bench.grid <- mlr3::benchmark_grid(
  task_list,
  learner.list,
  kfoldcv)
```

Puisque notre but est d’expliquer la visualisation, nous avons déjà calculé et sauvegardé les résultats dans `library(animint2data)`.
Le code ci-dessous télécharge ces résultats.

```{r}
if(TRUE){
  if(!requireNamespace("animint2data"))
    remotes::install_github("animint/animint2data")
  data(bench.result, package="animint2data")
}else{ # to re-run execute the two lines below:
  if(require(future))plan("multisession")
  bench.result <- mlr3::benchmark(bench.grid, store_models = TRUE)
}
```

Si vous souhaitez recalculer ces résultats, vous pouvez exécuter la ligne avec `benchmark()` ci-dessus.
<!-- comment -->
Le calcul pourrait prendre plusieurs minutes ou même plusieurs heures, en fonction de votre ordinateur.
<!-- comment -->
Le code `plan("multisession")` fait que chaque combinaison (données, algorithmes, divisions) est calculée sur un différent processeur.
<!-- comment -->
Pour aller encore plus vite, vous pouvez utiliser une grappe de calcul [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Graphiques d’erreur et d’aire sous la courbe {#test-error-and-accuracy-plots}

<!-- comment -->

Nous commençons avec la définition des mesures à calculer pour évaluer les prédictions.

<!-- comment -->

```{r}
test_measure_list <- mlr3::msrs(c(
  'classif.auc','classif.ce','classif.tpr','classif.fpr'))
sapply(test_measure_list, function(M)M$label)
```

<!-- comment -->

La sortie ci-dessous affiche la correspondance entre les noms et les codes pour les mesures.

* `classif.auc` est l’aire sous la courbe ROC.
* `classif.ce` est le taux d’erreur de classification, soit le nombre d’étiquettes avec prédictions incorrectes, divisé par le nombre d’étiquettes total.
* `classif.tpr` est le taux de vrais positifs, soit le nombre d’étiquettes positives avec prédictions correctes, divisé par le nombre d’étiquettes positives.
* `classif.fpr` est le taux de faux positifs, soit le nombre d’étiquettes négatives avec prédictions incorrectes, divisé par le nombre d’étiquettes négatives.

<!-- comment -->
Ensuite, nous utilisons `$score()` pour calculer ces mesures sur les données test, dans chaque division de validation croisée.

```{r}
score_dt <- bench.result$score(test_measure_list)
score_dt[, .(task_id, learner_id, iteration, classif.auc, classif.ce)]
```

La sortie ci-dessus contient une ligne pour chaque combinaison de données, algorithme, division de validation croisée.
Il y a des colonnes pour deux mesures : l’AUC et le taux d’erreur.
<!-- comment -->
Le code ci-dessous visualise les taux d’erreur.
<!-- comment -->
Remarquons que nous convertissons le taux d’erreur en pourcentage afin d’economiser l’espace sur l’axe X (par exemple, 0.3 devient 30).

```{r}
library(animint2)
score_dt[, let(percent_error=100*classif.ce)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    percent_error, learner_id),
    data=score_dt)+
  scale_x_continuous(
    breaks=seq(0,100,by=10),
    limits=c(0,60))
```

Le graphique ci-dessus affiche trois cercles pour chaque Y — un cercle pour chaque division de validation croisée.
<!-- comment -->
Nous voyons que l’algorithme des plus proches voisins a le plus petit taux d’erreur dans certains jeux de données (`vowel`), et le réseau de neurones (`torch_dense_50`) est meilleur dans d’autres données (`spam`).
<!-- comment -->
Le but dans ce chapitre est de rajouter l’interactivité à ce graphique, pour montrer des détails pour chaque division de validation croisée.

<!-- comment -->

### Courbes ROC {#roc-curves}

<!-- comment -->

Dans cette section, nous affichons des courbe ROC, qui sont utile pour l’évaluation dans la classification binaire.
<!-- comment -->
En fait, `$score()` a déjà calculé l’aire sous la courbe (AUC), que nous allons visualiser avec le code ci-dessous.

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt)
```

Le graphique ci-dessus affiche l’AUC sur l’ensemble test, pour chaque division de validation croisée, chaque jeu de données, et chaque algorithme d’apprentissage.
<!-- comment -->
Les résultats d’AUC suggèrent que `zip` est le plus facile, et `sonar` est le plus dificile, ce qui est consistent avec les taux d’erreur affichés dans le graphique précédent.
<!-- comment -->
Comme attendu, `featureless` (l’algorithme qui prédit toujours la classe majoritaire) a toujours AUC=0.5, ce qui correspond à une fonction de prédiction qui est constante. 
<!-- comment -->
Le code ci-dessous enlève les résultats pour `featureless`, pour souligner les autres résultats.

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt[learner_id!="featureless"])
```

Nous ne voyons plus les résultats pour `featureless` dans le graphique ci-dessus.
<!-- comment -->
Le code ci-dessous refait le même graphique, avec quelques additions pour l’affichage interactif.
<!-- comment -->
Remarquons que la colonne `task_it` sera utile pour une variable de sélection (pour à la fois `task_id` et `iteration`).

```{r}
score_dt[, task_it := paste(task_id, iteration)]
(gg.auc <- ggplot()+
   theme_bw()+
   facet_grid(task_id ~ ., scales="free")+
   scale_x_continuous(
     "Test set Area Under the ROC Curve")+
   geom_point(aes(
     classif.auc*100, learner_id),
     clickSelects="task_it",
     size=5,
     fill="grey",
     color="black",
     color_off="grey",
     data=score_dt[learner_id!="featureless"]))
```

Ensuite, nous calculons une courbe ROC pour chaque combinaison de `task_id`, `learner_id`, et division de validation croisée (`iteration`).
<!-- comment -->
Remarquons que le vecteur d’étiquettes à prédire dans la classification binaire est un facteur avec deux niveaux.
<!-- comment -->
Dans `mlr3`, le premier niveau est la classe positive, alors nous utilisons la première colonne de `prob` pour calculer la courbe ROC.

```{r}
roc_dt <- score_dt[, {
  pred <- prediction_test[[1]]
  is.positive <- as.integer(pred$truth)==1
  label01 <- ifelse(is.positive, 1, 0)
  WeightedROC::WeightedROC(pred$prob[,1], label01)
}, by=.(task_id, learner_id, iteration, task_it)]
roc_dt[, .(task_it, learner_id, threshold, FPR, TPR)]
```

Le tableau ci-dessus représente les courbes ROC à utiliser pour évaluer les prédictions sur les ensembles test.
<!-- comment -->
Nous voyons que `FPR=TPR=1` quand `threshold=0`, et `FPR=TPR=0` quand `threshold=Inf`.
<!-- comment -->
Le code ci-dessous affiche les courbes ROC dans différents facettes.

```{r}
gg.roc <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected="task_it")+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc+facet_grid(iteration ~ task_id)
```

Nous voyons une facette pour chaque jeu de données, et pour chaque division de validation croisée.
<!-- comment -->
Ensuite, nous rajoutons un point pour montrer le seuil par défaut pour chaque algorithme.

```{r}
gg.roc.point <- gg.roc+
  geom_point(aes(
    classif.fpr, classif.tpr, fill=learner_id,
    tooltip=sprintf(
      "%s default FPR=%.3f TPR=%.3f, errors=%.1f%%\n",
      learner_id, classif.fpr, classif.tpr, classif.ce*100)),
    data=score_dt,
    size=4,
    color="black",
    showSelected="task_it")
gg.roc.point+facet_grid(iteration ~ task_id)
```

Ci-dessus nous voyons un point pour le seuil par défaut.
<!-- comment -->
Il est évident que les différents algorithmes ont différents valeurs de `FPR` et `TPR`, par défaut.
<!-- comment -->
Ensuite nous rajoutons texte pour afficher la sélection de données et division de validation croisée.

```{r}
score_dt[, lev_str := {
  pred <- prediction_test[[1]]
  paste(levels(pred$truth), collapse=" vs ")
}, by=task_id]
gg.roc.text <- gg.roc.point+
  geom_text(aes(
    0.5, 0,
    label=sprintf(
      "%s (%s) fold %d",
      task_id, lev_str, iteration)),
    data=score_dt[learner_id=="featureless"],
    showSelected="task_it")
gg.roc.text+facet_grid(iteration ~ task_id)
```

Le graphique affiche le texte en bas de chaque facette.
<!-- comment -->
Ensuite, nous calculons le rang de chaque algorithme dans chaque jeu de données et division.

```{r}
score_dt[
, auc.it.rank := rank(classif.auc)
, by=.(task_id, iteration, task_it)]
```

Les rangs sont utilisés dans l’affichage ci-dessous.
<!-- comment -->
Nous créons une visualisation interactive à partir des graphiques de ROC et AUC.

```{r viz-auc-roc}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split")+
    theme_animint(width=400, height=400),
  roc=gg.roc.text+
    ggtitle("ROC curves for selection")+
    theme_animint(width=400, height=400)+
    geom_label_aligned(aes(
      Inf, auc.it.rank*0.1, color=learner_id,
      label=sprintf("%s AUC=%.3f", learner_id, classif.auc)),
      data=score_dt,
      hjust=1,
      showSelected="task_it",
      alignment="vertical"))
```

Dans la visualisation, vous pouvez cliquer les points d’AUC en haut, ce qui affichera les courbes ROC correspondantes en bas.

<!-- comment -->

### Interaction avec courbes ROC {#interactive-roc-curves}

<!-- comment -->

Dans la section précédente, nous avons affiché un point sur chaque courbe ROC pour représenter le seuil par défaut.
<!-- comment -->
Ces points correspondent aux différents taux de faux positifs, pour les différents algorithmes d’apprentissage.
<!-- comment -->
Pour une comparaison plus juste entre les algorithmes, nous pouvons afficher le meilleur taux de vrais positifs, pour un seuil de faux positifs (à définir par la sélection interactive).

```{r}
(best_TPR_dt <- roc_dt[, .(
  best_TPR=max(TPR)
), by=.(task_id,iteration,task_it,learner_id,FPR)])
```

Le tableau ci-dessous contient le meilleur taux de vrais positifs (`best_TPR`), pour chaque seuil de faux positifs (`FPR`).
<!-- comment -->
Nous voulons seulement afficher les seuils pour lesquelles il y a tous les algorithmes, alors nous contons le nombre d’algorithmes pour chaque seuil.

```{r}
best_TPR_algos <- best_TPR_dt[, .(
  algos = .N
), by=.(task_id,iteration,task_it,FPR)]
table(best_TPR_algos$algos)
```

Dans le tableau ci-dessus, nous voyons que

* il y a 30 seuils extrêmes avec tous les 5 algorithmes (y compris `featureless`),
* il y a plusieurs seuils avec seulement entre 1 et 3 algorithmes, que nous ne voulons pas afficher.
<!-- comment -->
Nous voulons seulement garder les seuils avec au moins 4 algorithmes, ce que nous faisons avec le code ci-dessous.
<!-- comment -->
En plus, pour que le nombre de seuils reste abordable pour la sélection interactive, nous imposons un maximum de 40 seuils, avec espacement presque régulier à travers les valeurs possible pour le taux de faux positifs.

```{r}
(min.algos <- length(learner.list)-1)
max.FPR.values <- 40
(FPR_vlines <- best_TPR_algos[algos >= min.algos][
, keep := c(TRUE, diff(ceiling(FPR*max.FPR.values))!=0)
, by=.(task_id,iteration,task_it)][keep==TRUE])
```

Le tableau ci-dessus contient les seuils que nous pouvons sélectionner avec l’interactivité.
<!-- comment -->
Dans le code ci-dessous, nous dessinons chaque seuil avec une ligne verticale.

```{r}
gg.roc.fpr <- gg.roc.text+
  geom_vline(aes(
    xintercept=FPR),
    alpha=0.7,
    size=5,
    data=FPR_vlines,
    showSelected="task_it",
    clickSelects="FPR")
gg.roc.fpr+facet_grid(iteration ~ task_id)
```

Le graphique ci-dessus affiche toutes les seuils, ce qui fait un fond noir.
<!-- comment -->
Ensuite, nous faisons un nouveau graphique pour afficher les meilleurs taux de vrais positifs, pour le seuil sélectionné.

```{r}
best_TPR_show <- best_TPR_dt[
  FPR_vlines, on=.(task_id,iteration,task_it,FPR)]
(gg.tpr <- ggplot()+
   theme_bw()+
   theme(legend.position="none")+
   geom_point(aes(
     best_TPR, learner_id, color=learner_id),
     data=best_TPR_show,
     size=5,
     showSelected=c("task_it","FPR"))+
   geom_text(aes(
     best_TPR, learner_id, label=sprintf("%.4f", best_TPR)),
     data=best_TPR_show,
     showSelected=c("task_it","FPR")))
```

Le graphique ci-dessus affiche un point et du texte pour les meilleurs taux de vrais positifs.
<!-- comment -->
Ensuite nous combinons tous les graphiques précédants dans une visualisation interactive.

```{r viz-select-fpr}
(viz.select.fpr <- animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  roc=gg.roc.fpr+
    ggtitle("ROC curves for selection")+
    theme_animint(last_in_row=TRUE),
  TPR=gg.tpr+
    ggtitle("Best TPR for selected FPR")+
    theme_animint(colspan=2, width=800, height=200)))
```

La visualisation ci-dessus permet de

<!-- comment -->

* cliquer le graphique en haut à gauche pour choisir un jeux de données et une division de validation croisée (`task_it`),
<!-- comment -->
* cliquer le graphique en haut à droite pour choisir un seuil de taux de faux positifs (`FPR`),
<!-- comment -->
* ensuite de voir les meilleurs taux de vrais positifs, dans le graphique en bas.

<!-- comment -->

Si vous cliquez les seuils dans les données `sonar`, vous allez voir que le meilleur algorithme dépende du choix de seuil.
<!-- comment -->
Par exemple, `cv_glmnet` est le pire pour `FPR` petit, mais il est le meilleur pour `FPR` plus grand.

<!-- comment -->

### Rajouter un niveau de zoom {#adding-a-zoom-level}

<!-- comment -->

Les visualisations précédentes utilisent facettes pour afficher l’AUC pour toutet les différents jeux de données.
<!-- comment -->
Dans cette section, nous proposons d’afficher l’AUC pour seulement un jeu de données à la fois.
<!-- comment -->
Nous allons faire un graphique qui permet la sélection d’un jeu de données en fonction du nombre d’observations, et du max AUC, qu’on calcule avec le code ci-dessous.

```{r}
(summary_dt <- score_dt[, .(
  max_auc=max(classif.auc),
  nrow=task[[1]]$nrow
), by=task_id])
```

Le tableau ci-dessus contient une ligne pour chaque jeu de données.
<!-- comment -->
Nous utilisons ce tableau pour dessiner le nuage de points ci-dessous.

```{r}
point_size <- 5
point_fill <- "grey"
point_color <- "black"
point_color_off <- "grey"
(gg.summary <- ggplot()+
  theme_bw()+
  geom_point(aes(
    nrow, max_auc),
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    clickSelects="task_id",
    data=summary_dt))
```

Le graphique contient un point pour chaque jeu de données.
<!-- comment -->
Remarquons dans le code ci-dessus que nous avons `clickSelects="task_id"`, ce qui permet la sélection d’un jeu de données seulement (pas la division de validation croisée).
<!-- comment -->
Ensuite nous créons un graphique avec `showSelected="task_id"` et `clickSelects="iteration"`.

```{r}
gg.auc.show.task <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt)
gg.auc.show.task+facet_grid(task_id ~ .)
```

Le graphique contient un panneau pour chaque jeu de données.
<!-- comment -->
Remarquons que les valeurs d’AUC sont très différent entre les jeux de données,et que `featureless` a toujours AUC=0.5, comme attendu.
<!-- comment -->
Nous supprimons `featureless` dans le graphique ci-dessous.

```{r}
gg.auc.show.task.zoom <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt[learner_id != "featureless"])
gg.auc.show.task.zoom+facet_grid(task_id ~ .)
```

Le graphique ci-dessus ne contient plus `featureless`.
<!-- comment -->
Ensuite nous faisons un graphique des courbes ROC pour la sélection de `task_id` et `iteration`.

```{r}
gg.roc.show.task.it <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected=c("task_id","iteration"))+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc.show.task.it+facet_grid(iteration ~ task_id)
```

Le graphique affiche un panneau pour chaque jeu de données, et pour chaque division de validation croisée.
<!-- comment -->
Le code ci-dessous combine ces graphiques dans une visualisation interactive.

```{r viz-two-selectors}
animint(
  summary=gg.summary+
    ggtitle("1. Select task"),
  roc=gg.roc.show.task.it+
    ggtitle("ROC curves for selected task and iteration")+
    theme_animint(last_in_row=TRUE),
  testAUC=gg.auc.show.task+
    ggtitle("2. Select iteration, all learners")+
    theme_animint(
      width=800, height=200, last_in_row=TRUE, colspan=2),
  testAUCzoom=gg.auc.show.task.zoom+
    ggtitle("2. Select iteration, zoom to non-trivial")+
    theme_animint(
      update_axes="x",
      width=800, height=200, last_in_row=TRUE, colspan=2))
```

Le graphique affiche

<!-- comment -->

* en haut à gauche, un résumé de tous les jeux de données (cliquez un point pour sélectionner un jeu de données).
<!-- comment -->
* en bas, deux graphiques d’AUC sur les ensembles test, pour le jeu de données sélectionné (cliquez un point pour sélectionner une division de validation croisée).
<!-- comment -->
* en haut à droite, les courbes ROC pour la sélection de `task_id` et `iteration`.

<!-- comment -->

### Tests statistiques pour différences entre algorithmes {#p-values-learners}

<!-- comment -->

Souvent nous voulons savoir s’il existe des différences significatives entre l’AUC de deux algorithmes (les deux avec AUC le plus grand).
Dans cette section, nous calculons des valeurs-p pour évaluer combien les différences sont significatives.
<!-- comment -->
Nous commençons avec le calcul de moyenne et écart type, sur les divisions de validation croisée.

```{r}
(score_stats <- dcast(
  score_dt[learner_id!="featureless"],
  task_id + learner_id ~ .,
  list(mean, sd),
  value.var="classif.auc"))
```

Nous visualisons ces statistiques avec le code ci-dessous.

```{r}
score_stats[, let(
  lo=classif.auc_mean-classif.auc_sd,
  hi=classif.auc_mean+classif.auc_sd)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc_mean, learner_id),
    data=score_stats)+
  geom_segment(aes(
    hi, learner_id,
    xend=lo, yend=learner_id),
    data=score_stats)+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)",
    breaks=seq(0,1,by=0.1))
```

Ci-dessus nous voyons un point pour chaque moyenne, et un segment pour l’écart type.
<!-- comment -->
Ensuite nous calculons le rang de chaque algorithme, pour chaque jeu de données.

```{r}
score_stats[, auc.task.rank := rank(-classif.auc_mean), by=task_id][]
```

Ci-dessus nous voyons une nouelle colonne `auc.task.rank` avec valeurs entre 1 et 4.
<!-- comment -->
Nous utilisons ces valeurs sur l’axe Y dans le graphique ci-dessous.

```{r}
gg.rank <- ggplot()+
  geom_point(aes(
    classif.auc_mean, auc.task.rank),
    data=score_stats)+
  geom_segment(aes(
    classif.auc_mean+classif.auc_sd, auc.task.rank,
    xend=classif.auc_mean-classif.auc_sd, yend=auc.task.rank),
    data=score_stats)+
  scale_y_continuous(
    "Algorithm rank using test AUC",
    breaks=1:4,
    limits=c(1,5))+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)")
gg.rank+facet_wrap("task_id", scales="free")
```

Ci-dessus nous voyons les rangs, mais nous ne voyons pas les noms.
<!-- comment -->
Nous rajoutons les noms ci-dessous.

```{r}
gg.rank.text <- gg.rank+
  geom_text(aes(
    -Inf, auc.task.rank,
    label=sprintf(
      "%s %.4f±%.4f",
      learner_id,
      classif.auc_mean, classif.auc_sd)),
    hjust=0, vjust=-0.5, size=10,
    data=score_stats)
gg.rank.text+facet_wrap("task_id", scales="free")
```

Pour tester les différences entre les rangs adjacents, nous faisons une jointure entre chaque rang (`auc.task.rank`) et le rang suivant (`next.rank`).

```{r}
score_dt_rank <- score_stats[, .(auc.task.rank, learner_id, task_id)][
  score_dt, on=.(learner_id, task_id), nomatch=0L
][order(task_id, auc.task.rank)]
(join_dt <- score_dt_rank[, next.rank := auc.task.rank+1][
  score_dt_rank, .(
    task_id, iteration,
    good.rank=i.auc.task.rank, bad.rank=x.auc.task.rank,
    good.auc =i.classif.auc,   bad.auc =x.classif.auc),
  on=.(task_id, iteration, auc.task.rank=next.rank),
  nomatch=0L])
```

Ensuite nous faisons des tests de Student.

```{r}
(pval_dt <- join_dt[, .SDcols=c("good.auc","bad.auc"), {
  mean_dt <- lapply(.SD, mean)
  paired <- t.test(
    bad.auc, good.auc, alternative = "l", paired=TRUE)
  c(mean_dt, p.paired=paired$p.value)
}, keyby=.(task_id, good.rank, bad.rank)])
```

Le tableau ci-dessus contient une ligne pour chaque test statistique.
<!-- comment -->
Ci-dessous, nous rajoutons des segments rouges pour souligner les différences entre moyennes.

```{r}
gg.rank.seg <- gg.rank+
  geom_segment(aes(
    bad.auc, good.rank+0.5,
    xend=good.auc, yend=good.rank+0.5),
    color="red",
    data=pval_dt)
gg.rank.seg+facet_wrap("task_id", scales="free")
```

Ci-dessus nous voyons un segment pour chaque test statistique.
<!-- comment -->
Ensuite nous rajoutons texte qui affiche les valeurs-p.

```{r}
gg.rank.p <- gg.rank.seg+
  geom_text(aes(
    bad.auc, good.rank+0.5, label=ifelse(
      p.paired<0.01, "P<0.01",
      sprintf("P=%.2f", p.paired))),
    color="red", hjust=1, size=10,
    data=pval_dt)
gg.rank.p+facet_wrap("task_id", scales="free")
```

Le graphique affiche les valeurs-p.

<!-- comment -->

* Une valeur-p est infériure à 1% (`P<0.01`), ce qui indique une différence significative, entre les rangs 2 et 3 dans les données `sonar`.
<!-- comment -->
* Les autres valeurs-p sont supérieure à 5% (`P>0.05`), ce qui indique des différences non-significatives.
<!-- comment -->
* L’absence des différences significatives est partiellement dûe à la taille de l’échantillon utilisé pour le test de Student (seulement 3 divisions de validation croisée). Vous pourriez eventuellement obtenir des valeurs-p plus petit en augmentant le nombre de divisions de validation croisée (exercise pour le lecteur).
<!-- comment -->
* Exercise : afficher les valeurs-p avec les noms des algorithmes et les statistiques (moyenne et écart type) sur le même graphique. Utilisez ce graphique au lieu de `testAUCzoom` dans la visualisation précédante.

<!-- comment -->

## Vérification de la régularisation {#over-under-fitting}

<!-- comment -->

Dans cette section, nous voulons créer des graphiques pour vérifier que les algorithmes sont régularisés correctement.
<!-- comment -->
Chaque algorithme d’apprentissage a une différente méthode pour la régularisation, avec différents hyper-paramètres pour éviter à la fois le surapprentissage et le sous-apprentissage.

<!-- comment -->

* Pour les plus proches voisins, nous augmentons le nombre de voisins pour régulariser. Un petit nombre de voisins peut donner un surapprentissage, et un grand nombre de voisins peut donner un sous-apprentissage.
<!-- comment -->
* Pour les algorithmes de `torch`, nous réduisons le nombre d’époques de descente de gradient pour régulariser. Un petit nombre d’époques peut donner un sous-apprentissage, et un grand nombre d’époques peut donner un surapprentissage.
<!-- comment -->
* Pour le modèle lineaire avec régularisation L1 (`glmnet`), nous augmentons la penalité pour régulariser. Une petite penalité peut donner un surapprentissage, et une grande penalité peut donner un sous-apprentissage.

<!-- comment -->

### Plus proches voisins {#nearest-neighbors-selection}

<!-- comment -->

Dans cette section, nous vérifions le choix de nombre de voisins.
Normalement, pour chaque division des données en test et entraînement, nous faisons ensuite une division d’entraînement en sous-entraînement et validation.
Nous voulons choisir le nombre de voisins qui maximise l’AUC sur l’ensemble de validation, et ensuite utiliser ce nombre de voisins pour calculer les prédictions sur l’ensemble test.
Pour chaque ensemble test, il pourrait avoir un différent nombre de voisins.
<!-- comment -->
Nous commençons par calculer un tableau avec une ligne par division de validation croisée, et une colonne avec le nombre de voisins.

```{r}
(best_dt <- score_dt[grep("knn", learner_id), {
  arch <- learner[[1]]$archive
  adata <- arch$data[, status := ifelse(
    classif.auc==max(classif.auc), "max", "other")]
  arch$best()[, .(k, details=list(adata))]
}, by=.(task_id, iteration, task_it)])
```

Le tableau contient les colonnes

<!-- comment -->

* `k`, le nombre de voisins choisi pour maximiser l’AUC sur l’ensemble de validation.
<!-- comment -->
* `details`, un tableau d’informations sur le choix du nombre de voisins.

<!-- comment -->

Ensuite, nous faisons un tableau qui combine les détails pour chaque division.

```{r}
details_dt <- best_dt[
, details[[1]][order(k)]
, by=.(task_id, iteration, task_it)]
details_dt[, .(task_id, iteration, k, classif.auc, status)]
```

Le tableau ci-dessus contient une ligne pour chaque nombre de voisins testé sur les ensembles de validation.
<!-- comment -->
Ensuite nous visualisons l’AUC sur l’ensemble de validation, en fonction du nombre de voisins.

```{r}
gg.neighbors <- ggplot()+
  geom_vline(aes(
    xintercept=k),
    showSelected="task_it",
    data=best_dt)+
  geom_point(aes(
    k, classif.auc, color=status),
    showSelected="task_it",
    data=details_dt)+
  scale_y_continuous("Validation set AUC")
gg.neighbors+facet_grid(task_id ~ iteration)
```

Dans le graphique ci-dessus, nous utilisons

* couleur pour souligner les meilleurs choix pour le nombre de voisins, et
* une ligne verticale pour indiquer le choix retenu.

Nous voyons que

<!-- comment -->

* Pour trois jeux de données (`sonar`, `spam`, `vowel`) nous avons max AUC avec un nombre de voisins intermédiare, ce qui indique un bon niveau de régularisation (ni surapprentissage, ni sous-apprentissage). Pour `vowel` itération 2, nous avons max AUC avec seulement 1 voisin, soit le plus petit hyper-paramètre dans notre recherche. D’habitude, ça pourrait suggèrer qu’il vaut mieux élargir l’espace de recherche, en espèrant qu’un hyper-paramètre encore plus petit soit meilleur. Cependent, 1 voisin est le plus petit nombre de voisins possible, alors ça ne pourrait pas être un sous-apprentissage, en nous n’avons donc pas besoin d’élargir notre recherche dans ce cas-ci.
<!-- comment -->
* Pour deux jeux de données (`waveform`, `zip`) nous avons max AUC avec le plus grand nombre de voisins (40), ce qui suggère un surapprentissage. Ces résultats suggèrent que nous pouvons eventuellement améliorer l’AUC en augmentant le nombre de voisins au-delà de 40. Idéalement, nous voyons l’AUC diminuer pour le max nombre de voisins, ce qui indique une recherche assez élargie.

<!-- comment -->

Ensuite nous faisons une visualisation interactive pour explorer les choix du nombre de voisins.

```{r viz-neighbors}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  knnDetails=gg.neighbors+
    ggtitle("Nearest neighbors model selection")+
    theme_animint(update_axes="y", last_in_row=TRUE))
```

Vous pouvez cliquer sur le graphique à gauche pour sélectionner un jeu de données et une division de validation croisée.
<!-- comment -->
Si vous cliquez sur `zip` ou `waveform`, nous voyons max AUC pour le plus grand nombre de voisins (40), ce qui suggère un surapprentissage.
Exercise : refaites l’apprentissage en augmentant le max nombre de voisins. Est-ce vous voyez le max AUC avec un nombre de voisins intermédiare ?

<!-- comment -->

### Torch {#torch}

<!-- comment -->

Dans cette section, nous explorons la régularisation d’arrêt prématuré, avec les algoritmes d’apprentissage dans le package `torch`.
Ces algorithmes utilisent la descente de gradient, ce qui minimise une fonction de perte sur l’ensemble de sous-entraînement.
Normalement la perte va aussi descendre sur l’ensemble de validation, jusqu’à un certain nombre d’époques, après quoi la perte va augmenter.
Nous voulons arrêter la descente de gradient quand la perte est minimale sur l’ensemble de validation, ce qui est « prématuré » car la perte va toujours descendre sur l’ensemble de sous-entraînement.
Nous allons vérifier que le nombre d’époques est choisi correctement, pour éviter le sous-apprentissage et le surapprentissage.
<!-- comment -->
Nous commençons avec le calcul d’une variable `best_epoch`, soit le nombre d’époques choisi (normalement le meilleur sur l’ensemble de validation).

```{r}
score_torch <- score_dt[
  grepl("torch",learner_id)
][, best_epoch := sapply(
  learner, function(L)unlist(L$tuning_result$internal_tuned_values)
)]
```

Ensuite, nous créons un tableau représentant l’histoire d’entraînement, avec une ligne pour chaque époque.

```{r}
(history_torch <- score_torch[, {
  L <- learner[[1]]
  M <- L$archive$learners(1)[[1]]$model
  M$torch_model_classif$model$callbacks$history
}, by=.(task_id, learner_id, iteration, task_it)])
```

Le tableau ci-dessus contient une ligne pour chaque époque, et les colonnes suivantes.

<!-- comment -->

* `train.classif.logloss`, la perte logistique (entropie croisée) sur l’ensemble utilisé pour calculer les gradients, ce qui devrait toujours diminuer, si les hyper-paramètres sont appropriés (taux d’apprentissage, taille de lot, et cætera). Remarquons que le nom `train` « entraînement » pourrait faire croire qu’il faudra encore diviser ces données en sous-entraînement et validation, mais ce n’est pas le cas. Pour préciser, nous allons plutôt utiliser le nom `subtrain` « sous-entraînement » dans les codes et graphiques ci-dessous.
<!-- comment -->
* `valid.classif.logloss`, la perte logistique (entropie croisée) sur l’ensemble qui n’est pas utilisé pour le calcul des gradients. Minimiser cette perte est la critère utilisée pour choisir le meilleur nombre d’époques, car `classif.logloss` était la première mesure dans `measure_list`.

<!-- comment -->

Ensuite, nous créons un tableau mieux adapté pour la visualisation.

```{r}
(history_long <- nc::capture_melt_single(
  history_torch,
  set=nc::alevels(valid="validation", train="subtrain"),
  ".classif.",
  measure=nc::alevels("logloss", auc="AUC")))
```

Le tableau ci-dessus contient les nouvelles colonnes `set`, `measure`, and `value`.
<!-- comment -->
Nous utilisons le code ci-dessous pour la visualisation d’un sous-ensemble.

```{r}
one_split <- function(DT,it=1)DT[iteration==it & task_id=="sonar"]
one_split_history <- one_split(history_long)
(gg.torch.line <- ggplot()+
  theme_bw()+
  facet_grid(measure ~ learner_id, labeller=label_both, scales="free")+
  geom_line(aes(
    epoch, value, color=set),
    data=one_split_history))
```

Le graphique ci-dessus contient des courbes d’apprentissage typique.

<!-- comment -->

* La perte logistique descende, et l’AUC augmente, pour l’ensemble de sous-apprentissage (utilisé pour le calcul des gradients).
<!-- comment -->
* La forme « U » est claire pour la perte sur l’ensemble de validation. Typiquement nous choissisons le nombre d’époques qui minimise cette courbe (ou bien, qui maximise l’AUC sur validation).

<!-- comment -->

Ensuite nous rajoutons points pour souligner les min et max.

```{r}
history_best <- history_long[, {
  mfun <- if(measure=="AUC")max else min
  .SD[value==mfun(value)]
}, by=.(task_id, iteration, task_it, learner_id, measure, set)
][, point := "best"]
one_split_best <- one_split(history_best)
(gg.torch.point <- gg.torch.line+
  geom_point(aes(
    epoch, value, color=set, fill=point),
    data=one_split_best)+
   scale_fill_manual(values=c(best="black")))
```

Dans le graphique ci-dessus, nous voyons que

<!-- comment -->

* il n’y a qu’un min pour chaque courbe de perte logistique.
<!-- comment -->
* il peut y avoir plus qu’un max pour chaque courbe d’AUC.
<!-- comment -->
* pour l’ensemble de sous-entraînement, l’AUC atteint rapidement le max de 1, mais la perte logistique continue à descendre.

<!-- comment -->

Ensuite nous rajoutons lignes verticales pour souligner le nombre d’époques choisi.

```{r}
one_split_score <- one_split(score_torch)
(gg.torch.text <- gg.torch.point+
  geom_vline(aes(
    xintercept=best_epoch),
    data=one_split_score)+
  geom_text(aes(
    best_epoch, -Inf, label=paste0(" selected epoch=", best_epoch)),
    vjust=-0.5, hjust=0,
    data=one_split_score))
```

Ci-dessus nous voyons que nous avons choisi le nombre d’époques qui minimise la perte logistique sur l’ensemble de validation.
<!-- comment -->
Ensuite, nous proposons un résumé, qui affiche le nombre d’époques pour chaque jeu de données, division de validation croisée, et algorithme d’apprentissage.

```{r}
(history_best_wide <- dcast(
  history_best,
  learner_id + set + task_id + iteration + task_it ~ measure,
  list(min, max, mid=function(x)(min(x)+max(x))/2),
  value.var="epoch"))
```

Le tableau contient colonnes pour les époques avec les meilleures mesures.
<!-- comment -->
Dans le graphique ci-dessous, nous utilisons les valeurs `mid`, soit à mi-chemin entre les min et max.

```{r}
history_best_valid <- history_best_wide[set=="validation"]
learner.colors <- c(
  torch_linear="grey",
  torch_dense_50="red")
(gg.best.valid <- ggplot()+
   theme_bw()+
   scale_fill_manual(values=learner.colors)+
   xlab("Epochs with best validation log loss")+
   ylab("Epochs with best validation AUC")+
   geom_point(aes(
     epoch_mid_logloss, epoch_mid_AUC, fill=learner_id),
     clickSelects="task_it", size=5,
     color_off="white", color="black",
     data=history_best_valid)+
   facet_wrap("task_id"))
```

Dans le graphique ci-dessus, il est clair que

<!-- comment -->

* la meilleure époque est plus petit que le max, pour quelques jeux de données (`sonar` and `waveform`). Ces résultats suggèrent un bon choix pour le nombre d’époques (ni surapprentissage, ni sous-apprentissage).
<!-- comment -->
* la meilleure époque est le max (200 époques), pour quelques jeux de données (`vowel` and `zip`). Ces résultats suggèrent le sous-apprentissage (nous pouvons augmenter le nombre d’époques pour minimiser davantage la perte sur l’ensemble de validation).

<!-- comment -->

Ensuite nous rajoutons un segment pour afficher le min et max nombre d’époques qui atteignent le meilleur AUC sur l’ensemble de validation.

```{r}
(gg.best.seg <- gg.best.valid+
   scale_color_manual(values=learner.colors)+
   geom_segment(aes(
     epoch_mid_logloss, epoch_min_AUC,
     xend=epoch_mid_logloss, yend=epoch_max_AUC,
     color=learner_id),
     size=3,
     clickSelects="task_it",
     data=history_best_valid))
```

Ci-dessus nous voyons qu’il y a plusieurs époques qui atteignent le max AUC, dans les ensembles validation de `vowel` et `zip`.
<!-- comment -->
Ensuite, nous préparons un graphique pour afficher les détails, avec des valeurs rélatives etre 0 et 1 (car les valeurs absolues sont très différentes entre les jeux de données).

```{r}
norm01 <- function(x)(x-min(x))/(max(x)-min(x))
history_long[
, relative_values := norm01(value)
, by=.(iteration, task_id, learner_id, set, measure)]
gg.log.auc <- ggplot()+
   theme_bw()+
   facet_grid(measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   geom_line(aes(
     epoch, relative_values, color=set, group=set),
     showSelected="task_it",
     data=history_long)
```

Enfin, nous combinons les graphiques pour créer une visualisation interactive.

```{r viz-torch}
(viz.torch <- animint(
  overview=gg.best.seg+
    theme_animint(width=800, colspan=2, last_in_row=TRUE)+
    ggtitle("Select task and cross-validation iteration"),
  details=gg.log.auc+
    ggtitle("Selected learning curves")))
```

Ci-dessous nous voyons deux graphiques_:

<!-- comment -->
* Cliquer le graphique en haut pour sélectionner un jeu de données et une division de validation croisée.
<!-- comment -->
* Le graphique en bas va ensuite afficher les courbes d’apprentissage pour la sélection.

<!-- comment -->

Pour certains jeux de données, l’AUC atteint rapidement un max proche de 1, et il est dificile de voir combien la courbe est proche de 1.
<!-- comment -->
Pour visualiser ces détails, nous calculons l’inverse de l’AUC (1-AUC).
<!-- comment -->
La meilleure valeur d’AUC est 1, mais la meilleure valeur pour l’inverse de l’AUC est 0.
<!-- comment -->
Alors nous pouvons utiliser l’échelle logarithmique, pour visualiser combien la courbe est proche de 0.

```{r}
history_long[, let(
  Measure = factor(
    ifelse(measure=="logloss", "logloss", "InvAUC"),
    c("logloss","InvAUC")),
  Relative_values = ifelse(
    measure=="logloss", relative_values, 1-relative_values))]
gg.log.scale <- ggplot()+
   theme_bw()+
   facet_grid(Measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   scale_y_log10()+
   geom_line(aes(
     epoch, Relative_values, color=set, group=set),
     showSelected=c("task_it", "set"),
     data=history_long)
```

Remarquons dans le code ci-dessus que nous avons utilisé les variables majuscules (`Measure` et `Relative_values`), avet l’échelle logarithmique pour Y.
<!-- comment -->
Ensuite nous rajoutons ce graphique à la visualisation interactive.

```{r viz-torch-log}
viz.torch.log <- viz.torch
viz.torch.log$detailsLog <- gg.log.scale+
  ggtitle("Selected learning curves (log scale)")+
  theme(legend.position="none")
viz.torch.log
```

La visualisation contient un graphique en bas à droite, qui affiche l’échelle logarithmique pour la sélection.
<!-- comment -->
En cliquant sur `vowel`, nous voyons des différences entre les graphiques de détails (en bas à droite, nous voyons mieux quelles époques sont les meilleures).

### glmnet {#glmnet}

Dans cette section, nous explorons visualisations de l’algorithme de `glmnet`, un modéle lineaire avec régularisation L1.
<!-- comment -->
Nous commençons avec la méthode `plot()` du premier modéle.

```{r}
score_glmnet <- score_dt[grep("glmnet",learner_id)]
L <- score_glmnet$learner[[1]]
library(glmnet)
plot(L$model)
score_glmnet[1, title(paste(task_id, iteration), line=3)]
```

Dans le graphique ci-dessus, nous voyons la perte logistique « Binomial Deviance » sur l’ensemble de validation, en fonction de la taille du modéle (nombre de non-zéro coefficients en haut, négative log penalité en bas).
<!-- comment -->
Nous voyons une courbe qui descende, mais qui ne rémonte pas, ce qui suggère le sous-apprentissage.
<!-- comment -->
Nous faisons une vérsion de ce graphique avec le code ci-dessous.

```{r}
cv_glmnet_one <- with(L$model, data.table(nzero, lambda, cvm, cvsd))
(gg.glmnet <- ggplot()+
   scale_y_continuous("Validation log loss")+
   geom_segment(aes(
     -log(lambda), cvm+cvsd,
     xend=-log(lambda), yend=cvm-cvsd),
     data=cv_glmnet_one)+
   geom_point(aes(
     -log(lambda), cvm, fill=nzero),
     data=cv_glmnet_one)+
   scale_fill_gradient(low="white", high="red"))
```

Ci-dessus nous voyons un ggplot qui ressemble au graphique précédant, avec la couleur indiquante le nombre de non-zéro coefficients.
<!-- comment -->
Ensuite, nous rajoutons du texte pour souligner le nombre de non-zéro coefficients pour plusieurs niveaux de régularisation.

```{r}
gg.glmnet+
  geom_text(aes(
    -log(lambda), Inf, label=nzero),
    vjust=1,
    data=cv_glmnet_one[seq(1, .N, by=5)])
```

Le graphique ci-dessus affiche le nombre de non-zéro coefficients en haut, pour plusieurs niveaux de régularisation.
<!-- comment -->
Notre but dans cette section est de faire une visualisation interactive avec ces informations.
<!-- comment -->
Dans le code ci-dessous, nous calculons un tableau de résultats pour tous les modéles.

```{r}
(cv_glmnet_all <- score_glmnet[, with(learner[[1]]$model, data.table(
  nzero, lambda, cvm, cvsd
)), by=.(task_id, iteration, task_it)])
```

Le tableau ci-dessus contient une ligne pour chaque jeu de données, chaque division de validation croisée, et chaque niveau de régularisation.
<!-- comment -->
Ensuite nous affichons la perte logistique sur l’ensemble de validation, avec une courbe pour chaque division de validation croisée, et un panneau pour chaque jeu de données.

```{r}
ggplot()+
  scale_y_continuous("Validation log loss")+
  geom_line(aes(
    -log(lambda), cvm, group=iteration),
    data=cv_glmnet_all)+
  facet_wrap("task_id", scales="free")
```

Ci-dessus nous voyons que le modéle lineaire risque le surapprentissage dans trois jeux de données (`sonar`, `vowel`, `waveform`), si le niveau de régularisation est trop faible.
Dans les autres jeux de données (`spam` and `zip`), nous voyons une courbe qui ne rémonte pas, ce qui suggère le sous-apprentissage.
Nous pourrions donc soupçonner qu’un modéle non-linéaire serait capable de donner des meilleurs résultats.
En regardant les résultats précédants, nous pourrions vérifier cette hypothèse.

* Dans `spam`, nous voyons que `torch_dense_50` obtient des meilleurs résultats que `cv_glmnet`. Nous en déduisons qu’une fonction lineaire n’est pas assez puissante pour obtenir la prédiction optimale dans ces données.
* Dans `zip`, nous voyons que tous les algorithmes d’apprentissage donnent une fonction de prédiction parfaite. Nous en déduisons que ces données répresentent un problème de prédiction qui est trés facile pour tous les algorithmes (même le modèle lineaire).

<!-- comment -->

## Résumé du chapitre et exercises {#ch20-exercises}

<!-- comment -->

Nous avons crée plusieurs visualisations des résultats d’apprentissage de `mlr3`.

<!-- comment -->

Exercises :

<!-- comment -->

* Dans `viz.select.fpr`, il n’y a pas de transitions fluides. Rajoutez-en une pour la variable `FPR`, et définissez `aes(key)` pour `geom_point()` et `geom_text()` dans `gg.tpr`.
<!-- comment -->
* Dans `viz.select.fpr`, un graphique affiche le meilleur taux de vrais positifs pour la sélection du taux de faux positifs. Enlevez ce graphique, et rajoutez les mêmes informations avec `geom_point()` et `geom_label_aligned()` dans le graphique avec les courbes ROC.
<!-- comment -->
* Le graphique `gg.summary` affiche seulement le max AUC. Rajoutez un `geom_segment()` qui affiche la variation d’AUC entre les algorithmes.
<!-- comment -->
* Le graphique `viz.select.fpr` affiche le meilleur taux de vrais positifs pour la sélection du taux de faux positifs.
<!-- comment -->
  * Rajoutez un graphique « Meilleur taux d’erreur pour la sélection du taux de faux positifs ». Indice : pour calculer le taux d’erreur, il faut faire une jointure entre le nombre d’étiquettes et les courbes ROC.
<!-- comment -->
  * Rajoutez un `geom_text()` qui affiche le taux séléctionné de faux positifs.
<!-- comment -->
  * Faites une visualisation qui affiche le meilleur taux de faux positifs, pour une sélection du taux de vrais positifs.
<!-- comment -->
* Dans les légendes, les deux modèles linéaires (`cv_glmnet` et `torch_linear`) ont différentes couleurs (jaune et violet). Accordez les deux couleurs (par exemple, violet pâle et violet foncé).
<!-- comment -->
* Dans les visualisations des courbes ROC, rajoutez `aes(color)` ou `aes(fill)` aux autres graphiques, avec une légende cohérente avec les courbes ROC. Affichez seulement une légende, avec les éléments triés pour accorder à l’ordre de présentation des axes Y affichant `learner_id`. Assurez-vous que vous pouvez cliquer sur un élément de la légende pour faire disparaître toutes les données correspondantes à cet algorithme d’apprentissage, à travers tous les graphiques.
<!-- comment -->
* Dans `viz.torch.log`, rajoutez des points pour mettre en évidence les meilleures époques, et une ligne verticale pour indiquer l’époque choisie par la validation croisée.
* Chaque section du chapitre a présenté sa propre méthode de visualisation. Combinez plusieurs méthodes dans une seule visualisation :
<!-- comment -->
  * Affichez tous les trois graphiques pour la vérification de la régularisation (`glmnet`, `torch`, nearest neighbors).
<!-- comment -->
  * Affichez les courbes ROC interactives à côté des graphiques pour la vérification de la régularisation.

<!-- comment -->

Ensuite, [l'annexe](/ch99) explique certaines méthodes de programmation R qui sont généralement utiles pour la visualisation de données.

