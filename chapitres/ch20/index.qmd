# Apprentissage artificiel avec `mlr3`

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="ch20-figures/")
if(FALSE){
  knitr::knit("index.qmd")
}
```

Dans ce chapitre, nous explorerons plusieurs visualisations pour comparer la performance des algorithmes d’apprentissage artificiel à l’aide des packages `mlr3`.

Plan du chapitre :
<!-- comment -->

* Nous démontrons d’abord la pertinence des packages `mlr3` pour la comparaison d’algorithmes d’apprentissage artificiel.
<!-- comment -->
* Nous utilisons ensuite `mlr3` pour définir et exécuter une évaluation comparative de plusieurs problèmes de classification binaire, et de plusieurs algorithmes d’apprentissage (plus proches voisins, modèles linéaires, réseaux de neurones).
<!-- comment -->
* Nous générons différents graphiques de taux d’erreur et d’aire sous la courbe ROC.
<!-- comment -->
* Nous ajoutons l’interactivité aux courbes ROC, pour présenter plusieurs niveaux de détail (données, itération de validation croisée, seuil des faux positifs).
<!-- comment -->
* Nous visualisons les mesures de fidélité et d’erreur par rapport à l’ensemble de validation, pour confirmer que le surapprentissage et le sous-apprentissage ont été évités.

<!-- comment -->

## Avantages de `mlr3` {#avantages-mlr3}

<!-- comment -->

Plusieurs packages R proposent des algorithmes d’apprentissage artificiel, par exemple :

<!-- comment -->

* `library(kknn)` offre des K Plus Proches Voisins (KPPV), présentés dans le [chapitre 10](/ch10).
<!-- comment -->
* `library(glmnet)` offre des modèles linéaires avec régularisation L1, comme le Lasso, exploré dans le [chapitre 11](/ch11).
<!-- comment -->
* `library(torch)` offre des réseaux de neurones, exposés dans le [chapitre 18](/ch18).

<!-- comment -->

Pour obtenir de bonnes prédictions sur un jeu de données, on doit entraîner plusieurs algorithmes, car il est impossible de savoir lequel sera le plus performant pour ces données.
<!-- comment -->
On utilise donc différents packages puis les résultats sont analysés pour déterminer quel algorithme fournit les meilleures prédictions.
<!-- comment -->
Les fonctions d’apprentissage et de prédiction ont des noms et des interfaces différents selon les packages.
<!-- comment -->
En général, chacun contient une fonction `predict()` qui calcule les prédictions, mais le type de sortie est différent d’un package à l’autre (`factor` pour classe, `matrix` pour probabilités de chaque classe, etc.).
<!-- comment -->


Comment adapter le jeu de données à chaque package d’apprentissage ?
<!-- comment -->
Comment uniformiser le type de sortie pour comparer les résultats ?
<!-- comment -->
Le package `mlr3` offre plusieurs fonctions qui simplifient ce type d’analyse, qu’on nomme « benchmark » [@bischl2024applied].

<!-- comment -->

* Chaque jeu de données est représenté par `Task`, c’est-à-dire un tableau de données accompagné de quelques métadonnées précisant le rôle de chaque colonne (entrée, sortie, etc.).
<!-- comment -->
* Chaque algorithme est représenté comme une classe `Learner` qui fournit les prédictions dans un type de sortie uniforme (une matrice de probabilités pour chaque classe).

<!-- comment -->

Dans ce chapitre, nous expliquons l’utilisation de `animint2` pour la visualisation interactive des résultats des algorithmes d’apprentissage sur différents jeux de données.
<!-- comment -->
Le code utilisé est une version modifiée d’un code pour la création des visualisations statiques [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Définition et exécution d’une analyse comparative {#exécution-comparaison}

<!-- comment -->

Dans `mlr3`, une comparaison benchmark comprend le calcul de toutes les combinaisons des éléments suivants :

<!-- comment -->

* une liste de jeux de données (`Tasks`) ;
<!-- comment -->
* une liste d’algorithmes d’apprentissage (`Learners`) ;
<!-- comment -->
* toutes les itérations d’une méthode de rééchantillonnage (`Resampling`).

<!-- comment -->

### Jeux de données (`Tasks`) {#tasks}

<!-- comment -->

Nous utilisons d’abord une liste de deux jeux de données.

```{r}
mlr.tasks <- c("spam","sonar")
task_list <- mlr3::tsks(mlr.tasks)
```

Nous définissons ensuite une fonction pour télécharger un jeu de données depuis le site web du livre *Elements of Statistical Learning* [@Hastie2009].

```{r}
library(data.table)
prefix <- "https://hastie.su.domains/ElemStatLearn/datasets/"
cache.fread <- function(data.name, f){
  cache.dir <- file.path("data", data.name)
  dir.create(cache.dir, showWarnings=FALSE, recursive=TRUE)
  local.path <- file.path(cache.dir, f)
  if(!file.exists(local.path)){
    u <- paste0(prefix, f)
    download.file(u, local.path)
  }
  fread(local.path)
}
```

Puis, nous créons une boucle pour télécharger trois jeux de données auxquels nous ajoutons un `Task` de classification binaire (en utilisant seulement les deux premières classes).

```r
data.sets <- c("vowel","waveform","zip")
for(data.name in data.sets){
  suffix <- if(data.name=="zip")".gz" else ""
  set.list <- list()
  for(predefined.set in c("test","train")){
    one.set.dt <- cache.fread(
      data.name,
      paste0(data.name, ".", predefined.set, suffix)
    )
    if("row.names" %in% names(one.set.dt)){
      one.set.dt[, row.names := NULL]
    }
    setnames(one.set.dt, old=names(one.set.dt)[1], new="y")
    set.list[[predefined.set]] <- one.set.dt
  }
  task.dt <- rbindlist(set.list)[
    y %in% unique(y)[1:2] #only first two classes.
  ][, y := factor(y)]
  one.task <- mlr3::TaskClassif$new(data.name, task.dt, target='y')
  task_list[[data.name]] <- one.task
}
task_list
```

Le code ci-dessus produit une liste de cinq tâches de classification binaire :

<!-- comment -->

* `spam` représente une classification de courriels (`spam` étant un pourriel et `nonspam` un courriel légitime) à partir d’un vecteur de comptage de mots « bag of words ».
<!-- comment -->
* [`sonar`](https://rdrr.io/cran/mlbench/man/Sonar.html) représente une classification entre métal et roche à partir d’un vecteur de bandes de fréquences sonores.
<!-- comment -->
* `vowel` représente une classification du son des voyelles, à partir de dix variables extraites d’un fichier audio.
<!-- comment -->
* `waveform` est un jeu de données de simulation [@Hastie2009].
<!-- comment -->
* `zip` représente une classification de chiffres manuscrits, à partir d’une image de 16x16 pixels, en échelle de gris.

<!-- comment -->

### Algorithmes d’apprentissage {#learners} 

<!-- comment -->

Dans cette section, nous définissons les algorithmes d’apprentissage à utiliser avec chaque jeu de données.
<!-- comment -->
Le code ci-dessous commence par une liste de deux algorithmes :

<!-- comment -->

* `cv_glmnet` est un modèle linéaire, avec régularisation L1 qui minimise le taux d’erreur dans la validation croisée à dix divisions (folds).
<!-- comment -->
* `featureless` est le modèle constant, il n’utilise pas les variables d’entrée, et prédit toujours la classe la plus fréquente dans les données d’apprentissage.

<!-- comment -->

```{r}
learner.list <- list(
  mlr3learners::LearnerClassifCVGlmnet$new()$configure(id="cv_glmnet"),
  mlr3::LearnerClassifFeatureless$new()$configure(id="featureless"))
```

Nous ajoutons ensuite l’algorithme des plus proches voisins.

```{r}
knn_learner <- mlr3learners::LearnerClassifKKNN$new()
knn_learner$param_set$values$k <- paradox::to_tune(1, 40)
```

Le code ci-dessus fixe les bornes de `k` (le nombre de voisins) entre 1 et 40.
<!-- comment -->
Le code qui suit précise la méthode de sélection de `k`, soit  une recherche avec dix valeurs, avec validation croisée à trois divisions, qui maximise l’aire sous la courbe ROC, (Area Under the Curve  ou AUC ).
<!-- comment -->
Pour calculer l’AUC pour l’ensemble de validation, il faut préciser `predict_type="prob"`, ce qui fait que la prédiction est un nombre réel (et non seulement la classe la plus probable).

```{r}
kfoldcv <- mlr3::rsmp("cv")
kfoldcv$param_set$values$folds <- 3
knn_learner$predict_type <- "prob"
learner.list$knn <- mlr3tuning::auto_tuner(
  learner = knn_learner,
  tuner = mlr3tuning::tnr("grid_search"),
  resampling = kfoldcv,
  measure = mlr3::msr("classif.auc", minimize = FALSE))
```

Ensuite, nous rajoutons deux algorithmes qui utilisent `library(torch)`.
<!-- comment -->
Nous employons une fonction qui renvoie une liste d’opérateurs servant à définir les algorithmes d’apprentissage.

```{r}
measure_list <- mlr3::msrs(c("classif.logloss", "classif.auc"))
n.epochs <- 200
make_po_list <- function(...)list(
  mlr3pipelines::po("scale"),
  mlr3pipelines::po(
    "select",
    selector = mlr3pipelines::selector_type(c("numeric", "integer"))),
  mlr3torch::PipeOpTorchIngressNumeric$new(),
  ...,
  mlr3pipelines::po("nn_head"),
  mlr3pipelines::po(
    "torch_loss",
    mlr3torch::t_loss("cross_entropy")),
  mlr3pipelines::po(
    "torch_optimizer",
    mlr3torch::t_opt("sgd", lr=0.1)),
  mlr3pipelines::po(
    "torch_callbacks",
    mlr3torch::t_clbk("history")),
  mlr3pipelines::po(
    "torch_model_classif",
    batch_size = 10,
    patience=n.epochs,
    measures_valid=measure_list,
    measures_train=measure_list,
    predict_type="prob",
    epochs = paradox::to_tune(upper = n.epochs, internal = TRUE)))
```

Dans le code ci-dessus `lr` est le taux d’apprentissage (learning rate).
<!-- comment -->
La fonction ci-dessous définit :

<!-- comment -->

* la moitié des données d’apprentissage comme ensemble de validation (`validate = 0.5`) et l’autre moitié comme ensemble de sous-entraînement (pour calculer les gradients).
<!-- comment -->
* le nombre d’époques choisi en minimisant la première mesure dans la liste `measure_list` (perte logistique).

```{r}
make_torch_learner <- function(id,...){
  po_list <- make_po_list(...)
  graph <- Reduce(mlr3pipelines::concat_graphs, po_list)
  glearner <- mlr3::as_learner(graph)
  mlr3::set_validate(glearner, validate = 0.5)
  mlr3tuning::auto_tuner(
    learner = glearner,
    tuner = mlr3tuning::tnr("internal"),
    resampling = mlr3::rsmp("insample"),
    measure = mlr3::msr("internal_valid_score", minimize = TRUE),
    term_evals = 1, id = id, store_models = TRUE)
}
```

Le code ci-dessous rajoute deux algorithmes d’apprentissage  :

```{r}
if(torch::torch_is_installed()){
  learner.list$linear <- make_torch_learner("torch_linear")
  learner.list$dense <- make_torch_learner(
    "torch_dense_50",
    mlr3pipelines::po(
      "nn_linear",
      out_features = 50),
    mlr3pipelines::po("nn_relu_1", inplace = TRUE))
}
```

### Calcul des résultats {#running-the-benchmark}

<!-- comment -->

Avant de calculer les résultats pour des algorithmes de classification, il faut préciser `predict_type=prob` pour que les prédictions soient des valeurs réelles (et pouvoir calculer des mesures d’évaluation comme l’AUC).

```{r}
for(learner.i in seq_along(learner.list)){
  learner.list[[learner.i]]$predict_type <- "prob"
}
```

Puis nous combinons les jeux de données, les algorithmes d’apprentissage et la méthode d’échantillonnage (validation croisée à trois divisions).

```{r}
bench.grid <- mlr3::benchmark_grid(
  task_list,
  learner.list,
  kfoldcv)
```

Puisque notre objectif est d’expliquer la visualisation, nous avons déjà calculé et sauvegardé les résultats dans `library(animint2data)`.
Le code ci-dessous télécharge ces résultats :

```{r}
if(TRUE){
  if(!requireNamespace("animint2data"))
    remotes::install_github("animint/animint2data")
  data(bench.result, package="animint2data")
}else{ # to re-run execute the two lines below:
  if(require(future))plan("multisession")
  bench.result <- mlr3::benchmark(bench.grid, store_models = TRUE)
}
```

Si vous souhaitez refaire le calcul, exécutez la ligne `benchmark()` ci-dessus.
<!-- comment -->
L’exécution pourrait durer plusieurs minutes, voire plusieurs heures, selon votre ordinateur.
<!-- comment -->
Le code `plan("multisession")` implique que chaque combinaison (données, algorithmes, divisions) est traitée par un processeur différent.
<!-- comment -->
Pour accélérer le processus, utilisez une grappe de calcul [@Hocking2025-mlr3-cluster].

<!-- comment -->

## Graphiques d’erreur et d’aire sous la courbe {#test-error-and-accuracy-plots}

<!-- comment -->

Tout d’abord, nous définissons les mesures à calculer pour évaluer les prédictions.

<!-- comment -->

```{r}
test_measure_list <- mlr3::msrs(c(
  'classif.auc','classif.ce','classif.tpr','classif.fpr'))
sapply(test_measure_list, function(M)M$label)
```

<!-- comment -->

La sortie ci-dessous affiche la correspondance entre le nom et le code des mesures.

* `classif.auc` : aire sous la courbe ROC.
* `classif.ce` : taux d’erreur de classification, soit le nombre d’étiquettes avec prédictions incorrectes, divisé par le nombre total d’étiquettes.
* `classif.tpr` : taux de vrais positifs, soit le nombre d’étiquettes positives avec prédictions correctes, divisé par le nombre d’étiquettes positives.
* `classif.fpr` : taux de faux positifs, soit le nombre d’étiquettes négatives avec prédictions incorrectes, divisé par le nombre d’étiquettes négatives.

<!-- comment -->
Ensuite, nous utilisons `$score()` pour calculer ces mesures pour les données test, dans chaque division de validation croisée.

```{r}
score_dt <- bench.result$score(test_measure_list)
score_dt[, .(task_id, learner_id, iteration, classif.auc, classif.ce)]
```

La sortie ci-dessus contient une ligne pour chaque combinaison données, algorithme, division de validation croisée et des colonnes pour deux mesures (AUC et taux d’erreur). 
<!-- comment -->
Le code ci-dessous visualise les taux d’erreur.
<!-- comment -->
Notez que nous convertissons le taux d’erreur en pourcentage afin d’économiser l’espace sur l’axe X (par exemple, 0.3 devient 30).

```{r}
library(animint2)
score_dt[, let(percent_error=100*classif.ce)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    percent_error, learner_id),
    data=score_dt)+
  scale_x_continuous(
    breaks=seq(0,100,by=10),
    limits=c(0,60))
```

Le graphique ci-dessus affiche trois cercles par ligne — un pour chaque division de validation croisée.
<!-- comment -->
On constate que l’algorithme des plus proches voisins a le plus petit taux d’erreur dans certains jeux de données (`vowel`), tandis que le réseau de neurones (`torch_dense_50`) est meilleur dans d’autres (`spam`).
<!-- comment -->
L’objectif de ce chapitre est d’ajouter l’interactivité à ce graphique, afin de montrer les détails de chaque division de validation croisée.

<!-- comment -->

### Courbes ROC {#roc-curves}

<!-- comment -->

Dans cette section, nous calculons des courbes ROC, utiles à l’évaluation en classification binaire.
<!-- comment -->
En fait, `$score()` a déjà calculé l’aire sous la courbe (AUC) et nous la visualisons avec le code ci-dessous :

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt)
```

Le graphique ci-dessus affiche l’AUC sur l’ensemble test, pour chaque division de validation croisée, jeu de données et algorithme d’apprentissage.
<!-- comment -->
Les résultats d’AUC suggèrent que `zip` est le plus facile à apprendre, et `sonar` le plus difficile, ce qui est cohérent avec les taux d’erreur affichés dans le graphique précédent.
<!-- comment -->
Comme prévu, `featureless` (l’algorithme qui prédit la classe majoritaire) a toujours AUC=0.5, ce qui correspond à une fonction de prédiction constante.
<!-- comment -->
Le code ci-dessous masque les résultats de `featureless`, pour mettre en évidence les autres résultats :

```{r}
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc, learner_id),
    data=score_dt[learner_id!="featureless"])
```

On n voit plus les résultats pour `featureless` dans le graphique ci-dessus.
<!-- comment -->
Le code ci-dessous produit le même graphique, avec quelques ajouts pour l’affichage interactif.
<!-- comment -->
Notez que la colonne `task_it` sera utile comme variable de sélection (à la fois pour `task_id` et pour `iteration`).

```{r}
score_dt[, task_it := paste(task_id, iteration)]
(gg.auc <- ggplot()+
   theme_bw()+
   facet_grid(task_id ~ ., scales="free")+
   scale_x_continuous(
     "Test set Area Under the ROC Curve")+
   geom_point(aes(
     classif.auc*100, learner_id),
     clickSelects="task_it",
     size=5,
     fill="grey",
     color="black",
     color_off="grey",
     data=score_dt[learner_id!="featureless"]))
```

Nous calculons ensuite une courbe ROC pour chaque combinaison de `task_id`, `learner_id` et division de validation croisée (`iteration`).
<!-- comment -->
Notez que le vecteur d’étiquettes à prédire dans la classification binaire est un facteur à deux niveaux.
<!-- comment -->
Dans `mlr3`, le premier niveau est la classe positive, nous utilisons donc la première colonne de `prob` pour calculer la courbe ROC.

```{r}
roc_dt <- score_dt[, {
  pred <- prediction_test[[1]]
  is.positive <- as.integer(pred$truth)==1
  label01 <- ifelse(is.positive, 1, 0)
  WeightedROC::WeightedROC(pred$prob[,1], label01)
}, by=.(task_id, learner_id, iteration, task_it)]
roc_dt[, .(task_it, learner_id, threshold, FPR, TPR)]
```

Le tableau ci-dessus représente les courbes ROC à utiliser pour évaluer les prédictions sur les ensembles tests.
<!-- comment -->
On constate que `FPR=TPR=1` quand `threshold=0`, et que `FPR=TPR=0` quand `threshold=Inf`.
<!-- comment -->
Le code ci-dessous affiche les courbes ROC dans des facettes distinctes.

```{r}
gg.roc <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected="task_it")+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc+facet_grid(iteration ~ task_id)
```

On voit une facette pour chaque jeu de données et pour chaque division de validation croisée.
<!-- comment -->
Nous ajoutons ci-dessous un point pour montrer le seuil par défaut pour chaque algorithme :

```{r}
gg.roc.point <- gg.roc+
  geom_point(aes(
    classif.fpr, classif.tpr, fill=learner_id,
    tooltip=sprintf(
      "%s default FPR=%.3f TPR=%.3f, errors=%.1f%%\n",
      learner_id, classif.fpr, classif.tpr, classif.ce*100)),
    data=score_dt,
    size=4,
    color="black",
    showSelected="task_it")
gg.roc.point+facet_grid(iteration ~ task_id)
```

Ci-dessus on voit un point pour le seuil par défaut.
<!-- comment -->
Il est évident que les algorithmes ont différentes valeurs de `FPR` et `TPR`, par défaut.
<!-- comment -->
Ci-dessous nous ajoutons du texte pour l’affichage de la sélection de données et de division de validation croisée :

```{r}
score_dt[, lev_str := {
  pred <- prediction_test[[1]]
  paste(levels(pred$truth), collapse=" vs ")
}, by=task_id]
gg.roc.text <- gg.roc.point+
  geom_text(aes(
    0.5, 0,
    label=sprintf(
      "%s (%s) fold %d",
      task_id, lev_str, iteration)),
    data=score_dt[learner_id=="featureless"],
    showSelected="task_it")
gg.roc.text+facet_grid(iteration ~ task_id)
```

Le graphique affiche le texte sous chaque facette.
<!-- comment -->
Nous calculons ensuite le rang des algorithmes dans chaque jeu de données et division.

```{r}
score_dt[
, auc.it.rank := rank(classif.auc)
, by=.(task_id, iteration, task_it)]
```

Les rangs calculés ci-dessus sont utilisés dans l’affichage qui suit.
<!-- comment -->
Nous créons une visualisation interactive à partir des graphiques de ROC et d’AUC.

```{r vis-auc-roc}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split")+
    theme_animint(width=400, height=400),
  roc=gg.roc.text+
    ggtitle("ROC curves for selection")+
    theme_animint(width=400, height=400)+
    geom_label_aligned(aes(
      Inf, auc.it.rank*0.1, color=learner_id,
      label=sprintf("%s AUC=%.3f", learner_id, classif.auc)),
      data=score_dt,
      hjust=1,
      showSelected="task_it",
      alignment="vertical"))
```

Cliquer sur les points d’AUC de la visualisation du haut, affiche les courbes ROC correspondantes en bas.

<!-- comment -->

### Interactivité des courbes ROC {#interactive-roc-curves}

<!-- comment -->

Dans la section précédente, nous avons affiché un point sur chaque courbe ROC pour représenter le seuil par défaut.
<!-- comment -->
Ces points correspondent aux différents taux de faux positifs.
<!-- comment -->
Pour améliorer la justesse de la comparaison entre les algorithmes, nous affichons le meilleur taux de vrais positifs, pour un seuil de faux positifs donné (à définir dans la sélection interactive).

```{r}
(best_TPR_dt <- roc_dt[, .(
  best_TPR=max(TPR)
), by=.(task_id,iteration,task_it,learner_id,FPR)])
```

Le tableau ci-dessous présente le meilleur taux de vrais positifs (`best_TPR`), pour chaque seuil de faux positifs (`FPR`).
<!-- comment -->
Nous voulons afficher uniquement les seuils pour lesquels tous les algorithmes sont présents, nous dénombrons donc les algorithmes pour chaque seuil.

```{r}
best_TPR_algos <- best_TPR_dt[, .(
  algos = .N
), by=.(task_id,iteration,task_it,FPR)]
table(best_TPR_algos$algos)
```

Notez que le tableau ci-dessus présente :

* 30 seuils extrêmes pour lesquels les cinq algorithmes sont présents (y compris `featureless`);
* plusieurs seuils avec trois algorithmes ou moins (nous ne voulons pas les afficher).
<!-- comment -->

Le code ci-dessous nous permet de conserver uniquement les seuils associés à au moins quatre algorithmes. 
<!-- comment -->
De plus, pour faciliter la sélection interactive, nous limitons le nombre de seuils à 40, répartis de manière quasi uniforme parmi les valeurs possibles pour le taux de faux positifs.

```{r}
(min.algos <- length(learner.list)-1)
max.FPR.values <- 40
(FPR_vlines <- best_TPR_algos[algos >= min.algos][
, keep := c(TRUE, diff(ceiling(FPR*max.FPR.values))!=0)
, by=.(task_id,iteration,task_it)][keep==TRUE])
```

Le tableau ci-dessus regroupe les seuils sélectionnables avec l’interactivité.
<!-- comment -->
Dans le code ci-dessous, nous représentons chaque seuil par une ligne verticale :

```{r}
gg.roc.fpr <- gg.roc.text+
  geom_vline(aes(
    xintercept=FPR),
    alpha=0.7,
    size=5,
    data=FPR_vlines,
    showSelected="task_it",
    clickSelects="FPR")
gg.roc.fpr+facet_grid(iteration ~ task_id)
```

Le graphique ci-dessus affiche une ligne verticale pour chaque seuil, si bien qu’il apparaît entièrement noir.
<!-- comment -->
Nous créons ensuite un nouveau graphique pour afficher les meilleurs taux de vrais positifs, pour le seuil sélectionné.

```{r}
best_TPR_show <- best_TPR_dt[
  FPR_vlines, on=.(task_id,iteration,task_it,FPR)]
(gg.tpr <- ggplot()+
   theme_bw()+
   theme(legend.position="none")+
   geom_point(aes(
     best_TPR, learner_id, color=learner_id),
     data=best_TPR_show,
     size=5,
     showSelected=c("task_it","FPR"))+
   geom_text(aes(
     best_TPR, learner_id, label=sprintf("%.4f", best_TPR)),
     data=best_TPR_show,
     showSelected=c("task_it","FPR")))
```

Le graphique ci-dessus affiche un point et du texte pour les meilleurs taux de vrais positifs.
<!-- comment -->
Nous combinons finalement tous les graphiques précédents dans une visualisation interactive.

```{r vis-select-fpr}
(vis.select.fpr <- animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  roc=gg.roc.fpr+
    ggtitle("ROC curves for selection")+
    theme_animint(last_in_row=TRUE),
  TPR=gg.tpr+
    ggtitle("Best TPR for selected FPR")+
    theme_animint(colspan=2, width=800, height=200)))
```

La visualisation ci-dessus permet de :

<!-- comment -->

* cliquer sur le graphique en haut à gauche pour choisir un jeu de données et une division de validation croisée (`task_it`) ;
<!-- comment -->
* cliquer sur le graphique en haut à droite pour choisir un seuil de taux de faux positifs (`FPR`) ;
<!-- comment -->
* visualiser les meilleurs taux de vrais positifs, dans le graphique du bas.

<!-- comment -->

En cliquant sur les seuils de FPR dans les données `sonar`, vous verrez que le meilleur algorithme dépend du seuil choisi.
<!-- comment -->
Par exemple, `cv_glmnet` est le moins bon pour un `FPR` faible, mais il est le meilleur pour un `FPR` plus élevé.

<!-- comment -->

### Ajout d’un niveau de zoom {#adding-a-zoom-level}

<!-- comment -->

Les visualisations précédentes recouraient à des facettes pour afficher l’AUC des différents jeux de données.
<!-- comment -->
Dans cette section, nous proposons d’afficher l’AUC pour un seul jeu à la fois.
<!-- comment -->
Nous créerons un graphique permettant de sélectionner un jeu de données en fonction du nombre d’observations et de l’AUC maximale, valeurs que nous calculons avec le code ci-dessous :

```{r}
(summary_dt <- score_dt[, .(
  max_auc=max(classif.auc),
  nrow=task[[1]]$nrow
), by=task_id])
```

Le tableau ci-dessus contient une ligne par jeu de données.
<!-- comment -->
Nous l’utilisons pour dessiner le nuage de points ci-dessous :

```{r}
point_size <- 5
point_fill <- "grey"
point_color <- "black"
point_color_off <- "grey"
(gg.summary <- ggplot()+
  theme_bw()+
  geom_point(aes(
    nrow, max_auc),
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    clickSelects="task_id",
    data=summary_dt))
```

Le graphique contient un point pour chaque jeu de données.
<!-- comment -->
Notez que dans le code ci-dessus, le paramètre `clickSelects="task_id"` permet uniquement la sélection d’un jeu de données  (et non de la division de validation croisée).
<!-- comment -->
Nous créons ensuite un graphique avec `showSelected="task_id"` et `clickSelects="iteration"`.

```{r}
gg.auc.show.task <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt)
gg.auc.show.task+facet_grid(task_id ~ .)
```

Le graphique ci-dessus présente un panneau par jeu de données.
<!-- comment -->
On constate que les valeurs d’AUC varient énormément d’un jeu de données à l’autre, et que `featureless` a toujours une AUC de 0.5, comme prévu.
<!-- comment -->
Nous supprimons `featureless` dans le graphique ci-dessous :

```{r}
gg.auc.show.task.zoom <- ggplot()+
  theme_bw()+
  geom_point(aes(
    classif.auc, learner_id),
    clickSelects="iteration",
    showSelected="task_id",
    size=point_size,
    fill=point_fill,
    color=point_color,
    color_off=point_color_off,
    data=score_dt[learner_id != "featureless"])
gg.auc.show.task.zoom+facet_grid(task_id ~ .)
```

Le graphique ci-dessus exclut `featureless`.
<!-- comment -->
Nous créons maintenant un graphique des courbes ROC pour la sélection de `task_id` et de `iteration`.

```{r}
gg.roc.show.task.it <- ggplot()+
  theme_bw()+
  geom_path(aes(
    FPR, TPR, group=learner_id, color=learner_id),
    data=roc_dt,
    showSelected=c("task_id","iteration"))+
  scale_x_continuous(breaks=seq(0,1,by=0.5))+
  scale_y_continuous(breaks=seq(0,1,by=0.5))
gg.roc.show.task.it+facet_grid(iteration ~ task_id)
```

Le graphique affiche un panneau par jeu de données et par division de validation croisée.
<!-- comment -->
Le code ci-dessous combine ces graphiques dans une visualisation interactive :

```{r vis-two-selectors}
animint(
  summary=gg.summary+
    ggtitle("1. Select task"),
  roc=gg.roc.show.task.it+
    ggtitle("ROC curves for selected task and iteration")+
    theme_animint(last_in_row=TRUE),
  testAUC=gg.auc.show.task+
    ggtitle("2. Select iteration, all learners")+
    theme_animint(
      width=800, height=200, last_in_row=TRUE, colspan=2),
  testAUCzoom=gg.auc.show.task.zoom+
    ggtitle("2. Select iteration, zoom to non-trivial")+
    theme_animint(
      update_axes="x",
      width=800, height=200, last_in_row=TRUE, colspan=2))
```

Le graphique affiche :

<!-- comment -->

* en haut à gauche, un résumé des jeux de données (cliquez sur un point pour sélectionner un jeu);
<!-- comment -->
* en bas, deux graphiques de l’AUC pour les ensembles tests pour le jeu de données sélectionné (cliquez sur un point pour sélectionner une division de validation croisée);
<!-- comment -->
* en haut à droite, les courbes ROC pour la sélection de `task_id` et de `iteration`.

<!-- comment -->

### Tests statistiques des différences entre les algorithmes {#p-values-learners}

<!-- comment -->

Il est souvent utile de déterminer si des différences significatives existent entre les AUC des deux algorithmes ayant les AUC les plus grandes.
Dans cette section, nous calculons des valeurs-p pour évaluer si les différences sont significatives.
<!-- comment -->
Nous calculons d’abord la moyenne et l’écart type des divisions de validation croisée.

```{r}
(score_stats <- dcast(
  score_dt[learner_id!="featureless"],
  task_id + learner_id ~ .,
  list(mean, sd),
  value.var="classif.auc"))
```

Nous visualisons ces statistiques grâce au code ci-dessous :

```{r}
score_stats[, let(
  lo=classif.auc_mean-classif.auc_sd,
  hi=classif.auc_mean+classif.auc_sd)]
ggplot()+
  facet_grid(task_id ~ .)+
  geom_point(aes(
    classif.auc_mean, learner_id),
    data=score_stats)+
  geom_segment(aes(
    hi, learner_id,
    xend=lo, yend=learner_id),
    data=score_stats)+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)",
    breaks=seq(0,1,by=0.1))
```

Ci-dessus on voit un point pour chaque moyenne et un segment pour l’écart type.
<!-- comment -->
Nous calculons ensuite le rang des algorithmes pour chaque jeu de données.

```{r}
score_stats[, auc.task.rank := rank(-classif.auc_mean), by=task_id][]
```

Ci-dessus on constate la présence d’une nouvelle colonne `auc.task.rank` qui présente des valeurs entre 1 et 4.
<!-- comment -->
Nous utilisons ces valeurs sur l’axe Y dans le graphique ci-dessous :

```{r}
gg.rank <- ggplot()+
  geom_point(aes(
    classif.auc_mean, auc.task.rank),
    data=score_stats)+
  geom_segment(aes(
    classif.auc_mean+classif.auc_sd, auc.task.rank,
    xend=classif.auc_mean-classif.auc_sd, yend=auc.task.rank),
    data=score_stats)+
  scale_y_continuous(
    "Algorithm rank using test AUC",
    breaks=1:4,
    limits=c(1,5))+
  scale_x_continuous(
    "Test AUC (mean ± SD over 3 folds in CV)")
gg.rank+facet_wrap("task_id", scales="free")
```

Ci-dessus on voit bien les rangs, mais pas les noms.
<!-- comment -->
Nous les ajoutons ci-dessous :

```{r}
gg.rank.text <- gg.rank+
  geom_text(aes(
    -Inf, auc.task.rank,
    label=sprintf(
      "%s %.4f±%.4f",
      learner_id,
      classif.auc_mean, classif.auc_sd)),
    hjust=0, vjust=-0.5, size=10,
    data=score_stats)
gg.rank.text+facet_wrap("task_id", scales="free")
```

Pour tester les différences entre les rangs adjacents, nous effectuons une jointure entre chaque rang (`auc.task.rank`) et le suivant (`next.rank`).

```{r}
score_dt_rank <- score_stats[, .(auc.task.rank, learner_id, task_id)][
  score_dt, on=.(learner_id, task_id), nomatch=0L
][order(task_id, auc.task.rank)]
(join_dt <- score_dt_rank[, next.rank := auc.task.rank+1][
  score_dt_rank, .(
    task_id, iteration,
    good.rank=i.auc.task.rank, bad.rank=x.auc.task.rank,
    good.auc =i.classif.auc,   bad.auc =x.classif.auc),
  on=.(task_id, iteration, auc.task.rank=next.rank),
  nomatch=0L])
```

Nous appliquons ensuite des tests de Student.

```{r}
(pval_dt <- join_dt[, .SDcols=c("good.auc","bad.auc"), {
  mean_dt <- lapply(.SD, mean)
  paired <- t.test(
    bad.auc, good.auc, alternative = "l", paired=TRUE)
  c(mean_dt, p.paired=paired$p.value)
}, keyby=.(task_id, good.rank, bad.rank)])
```

Le tableau ci-dessus comporte une ligne par test statistique.
<!-- comment -->
Ci-dessous, nous rajoutons des segments en rouge pour souligner les différences entre les moyennes.

```{r}
gg.rank.seg <- gg.rank+
  geom_segment(aes(
    bad.auc, good.rank+0.5,
    xend=good.auc, yend=good.rank+0.5),
    color="red",
    data=pval_dt)
gg.rank.seg+facet_wrap("task_id", scales="free")
```

Ci-dessus nous voyons un segment pour chaque test statistique.
<!-- comment -->
Nous ajoutons ensuite des éléments de texte pour afficher les valeurs-p.

```{r}
gg.rank.p <- gg.rank.seg+
  geom_text(aes(
    bad.auc, good.rank+0.5, label=ifelse(
      p.paired<0.01, "P<0.01",
      sprintf("P=%.2f", p.paired))),
    color="red", hjust=1, size=10,
    data=pval_dt)
gg.rank.p+facet_wrap("task_id", scales="free")
```

Le graphique affiche les valeurs-p.

<!-- comment -->

* Une des valeurs-p est inférieure à 1% (`P<0.01`), ce qui indique une différence significative entre les rangs 2 et 3 dans les données `sonar`.
<!-- comment -->
* Les autres valeurs-p sont supérieures à 5% (`P>0.05`), ce qui indique des différences non significatives.
<!-- comment -->
* L’absence de différences significatives s’explique en partie par la taille réduite de l’échantillon utilisé pour le test de Student (seulement trois divisions de validation croisée). On pourrait obtenir des valeurs-p plus petites en augmentant le nombre de divisions de validation croisée (voir exercice).
<!-- comment -->

Exercice : combinez sur un même graphique les valeurs-p avec le nom des algorithmes et les statistiques (moyenne et écart type). Utilisez ce graphique au lieu de `testAUCzoom` dans la visualisation précédente.

<!-- comment -->

## Vérification de la régularisation {#over-under-fitting}

<!-- comment -->

Dans cette section, nous créerons des graphiques pour vérifier que les algorithmes sont régularisés correctement.
<!-- comment -->
Chaque algorithme d’apprentissage utilise une méthode de régularisation différente, et des hyperparamètres spécifiques pour éviter à la fois le surapprentissage et le sous-apprentissage.

<!-- comment -->

* Pour les plus proches voisins, il faut augmenter le nombre de voisins pour régulariser. Un petit nombre de voisins peut donner un surapprentissage; un grand nombre, un sous-apprentissage.
<!-- comment -->
* Pour les algorithmes de `torch`, il faut réduire le nombre d’époques de descente de gradient pour régulariser. Un petit nombre d’époques peut donner un sous-apprentissage; un grand nombre, un surapprentissage.
<!-- comment -->
* Pour le modèle linéaire avec régularisation L1 (`glmnet`), il faut augmenter la pénalité pour régulariser. Une petite pénalité peut donner un surapprentissage; une grande pénalité, un sous-apprentissage.

<!-- comment -->

### Plus proches voisins {#nearest-neighbors-selection}

<!-- comment -->

Dans cette section, nous vérifions le choix du nombre de voisins.
Normalement, après avoir divisé les données en ensembles de test et d’entraînement, nous subdivisons l’ensemble d’entraînement en sous-entraînement et validation.
L’objectif est de retenir le nombre de voisins qui maximise l’AUC sur l’ensemble de validation, et d’utiliser ce nombre pour le calcul des prédictions sur l’ensemble test.
Pour chaque ensemble test, il pourrait y avoir un nombre différent de voisins.
<!-- comment -->
Nous commençons par calculer un tableau avec une ligne par division de validation croisée, et une colonne pour le nombre de voisins.

```{r}
(best_dt <- score_dt[grep("knn", learner_id), {
  arch <- learner[[1]]$archive
  adata <- arch$data[, status := ifelse(
    classif.auc==max(classif.auc), "max", "other")]
  arch$best()[, .(k, details=list(adata))]
}, by=.(task_id, iteration, task_it)])
```

Le tableau contient les colonnes suivantes :

<!-- comment -->

* `k`, le nombre de voisins choisi pour maximiser l’AUC sur l’ensemble de validation;
<!-- comment -->
* `details`, un tableau d’informations sur le choix du nombre de voisins.

<!-- comment -->

Ensuite, nous bâtissons un tableau combinant les détails de chaque division.

```{r}
details_dt <- best_dt[
, details[[1]][order(k)]
, by=.(task_id, iteration, task_it)]
details_dt[, .(task_id, iteration, k, classif.auc, status)]
```

Le tableau ci-dessus contient une ligne pour chaque nombre de voisins testé sur les ensembles de validation.
<!-- comment -->
Nous visualisons ensuite l’AUC sur l’ensemble de validation, en fonction du nombre de voisins.

```{r}
gg.neighbors <- ggplot()+
  geom_vline(aes(
    xintercept=k),
    showSelected="task_it",
    data=best_dt)+
  geom_point(aes(
    k, classif.auc, color=status),
    showSelected="task_it",
    data=details_dt)+
  scale_y_continuous("Validation set AUC")
gg.neighbors+facet_grid(task_id ~ iteration)
```

Dans le graphique ci-dessus :

* la couleur met en évidence les meilleurs choix pour le nombre de voisins;
* une ligne verticale indique le choix retenu.

L’examen du graphique révèle que :

<!-- comment -->

* Trois jeux de données (`sonar`, `spam`, `vowel`) présentent une AUC maximale avec un nombre de voisins intermédiaire, ce qui indique un bon niveau de régularisation (ni surapprentissage ni sous-apprentissage). Pour `vowel` itération 2, l’AUC est maximale avec seulement 1 voisin, soit le plus petit hyperparamètre de la recherche. En principe, ce résultat inviterait à élargir l’espace de recherche, dans l’espoir qu’un hyperparamètre plus petit soit meilleur. Cependant, puisque 1 voisin est le plus petit nombre possible, il ne peut s’agir de sous-apprentissage ; il est donc inutile d’élargir la recherche.
<!-- comment -->
* Deux jeux de données (`waveform`, `zip`) présentent une AUC maximale avec le plus grand nombre de voisins (40), ce qui suggère un surapprentissage. Ces résultats laissent supposer que l’AUC pourrait être améliorée si le nombre de voisins était augmenté au-delà de 40. La situation idéale serait une AUC qui diminue pour le nombre maximal de voisins, ce qui indiquerait une recherche suffisamment élargie.

<!-- comment -->

Nous créons ensuite une visualisation interactive pour comprendre le choix du nombre de voisins.

```{r vis-neighbors}
animint(
  testAUC=gg.auc+
    ggtitle("Select data set and cross-validation split"),
  knnDetails=gg.neighbors+
    ggtitle("Nearest neighbors model selection")+
    theme_animint(update_axes="y", last_in_row=TRUE))
```

Le graphique de gauche permet de sélectionner un jeu de données et une division de validation croisée.
<!-- comment -->
Si vous cliquez sur `zip` ou sur `waveform`, vous observerez une AUC maximale pour le plus grand nombre de voisins (40), ce qui suggère un surapprentissage.
Exercice : relancez l’apprentissage en augmentant le max nombre de voisins. Est-ce que vous voyez le max AUC avec un nombre de voisins intermédiaire ?

<!-- comment -->

### Torch {#torch}

<!-- comment -->

Dans cette section, nous explorons la régularisation d’arrêt prématuré appliquée aux algorithmes d’apprentissage du package `torch`.
Ces algorithmes utilisent la descente de gradient, ce qui minimise une fonction de perte sur l’ensemble de sous-entraînement.
En règle générale, la perte sur l’ensemble de validation décroit jusqu’à un certain nombre d’époques, pour ensuite augmenter.
L’objectif est d’interrompre la descente de gradient au moment où la perte atteint son minimum sur l’ensemble de validation, ce qui est « prématuré », car la perte continue de descendre sur l’ensemble de sous-entraînement. 
Nous allons vérifier que le nombre d’époques retenu évite le sous-apprentissage et le surapprentissage.
<!-- comment -->
Nous calculons d’abord une variable `best_epoch`, soit le nombre d’époques choisi (normalement le meilleur sur l’ensemble de validation).

```{r}
score_torch <- score_dt[
  grepl("torch",learner_id)
][, best_epoch := sapply(
  learner, function(L)unlist(L$tuning_result$internal_tuned_values)
)]
```

Nous créons ensuite un tableau représentant l’historique d’entraînement, avec une ligne par époque.

```{r}
(history_torch <- score_torch[, {
  L <- learner[[1]]
  M <- L$archive$learners(1)[[1]]$model
  M$torch_model_classif$model$callbacks$history
}, by=.(task_id, learner_id, iteration, task_it)])
```

Le tableau ci-dessus contient une ligne par époque, et les colonnes suivantes :

<!-- comment -->

* `train.classif.logloss`, la perte logistique (entropie croisée) sur l’ensemble utilisé pour calculer les gradients, servant au calcul des gradients, elle devrait toujours diminuer si les hyperparamètres sont appropriés (taux d’apprentissage, taille de lot, etc.). Notez que le nom `train` (entraînement) pourrait faire croire qu’il faudra encore diviser ces données en sous-entraînement et validation, mais ce n’est pas le cas. Pour éviter toute confusion, nous allons plutôt utiliser le nom `subtrain` (sous-entraînement) dans les codes et les graphiques qui suivent.
<!-- comment -->
* `valid.classif.logloss`, la perte logistique (entropie croisée) sur l’ensemble qui ne sert pas au calcul des gradients. Minimiser cette perte est le critère utilisé pour choisir le meilleur nombre d’époques, car `classif.logloss` était la première mesure dans `measure_list`.

<!-- comment -->

Nous créons maintenant un tableau mieux adapté à la visualisation.

```{r}
(history_long <- nc::capture_melt_single(
  history_torch,
  set=nc::alevels(valid="validation", train="subtrain"),
  ".classif.",
  measure=nc::alevels("logloss", auc="AUC")))
```

Le tableau ci-dessus contient les nouvelles colonnes `set`, `measure` et `value`.
<!-- comment -->
Nous utilisons le code ci-dessous pour la visualisation d’un sous-ensemble :

```{r}
one_split <- function(DT,it=1)DT[iteration==it & task_id=="sonar"]
one_split_history <- one_split(history_long)
(gg.torch.line <- ggplot()+
  theme_bw()+
  facet_grid(measure ~ learner_id, labeller=label_both, scales="free")+
  geom_line(aes(
    epoch, value, color=set),
    data=one_split_history))
```

Le graphique ci-dessus présente des courbes d’apprentissage typiques :

<!-- comment -->

* La perte logistique diminue et l’AUC augmente  pour l’ensemble de sous-apprentissage (utilisé dans le calcul des gradients).
<!-- comment -->
* Comme prévu, la perte sur l’ensemble de validation diminue d’abord et remonte ensuite. Généralement, le nombre d’époques choisi est celui qui minimise cette courbe (ou bien, qui maximise l’AUC pour l’ensemble de validation).

<!-- comment -->

Nous ajoutons maintenant des points pour mettre en évidence le minimum et le maximum.

```{r}
history_best <- history_long[, {
  mfun <- if(measure=="AUC")max else min
  .SD[value==mfun(value)]
}, by=.(task_id, iteration, task_it, learner_id, measure, set)
][, point := "best"]
one_split_best <- one_split(history_best)
(gg.torch.point <- gg.torch.line+
  geom_point(aes(
    epoch, value, color=set, fill=point),
    data=one_split_best)+
   scale_fill_manual(values=c(best="black")))
```

Le graphique ci-dessus révèle des caractéristiques importantes :

<!-- comment -->

* il n’y a qu’un minimum pour chaque courbe de perte logistique ;
<!-- comment -->
* il peut y avoir plus d’un maximum d’AUC ;
<!-- comment -->
* pour l’ensemble de sous-entraînement, l’AUC atteint rapidement le maximum (1), mais la perte logistique continue de diminuer.

<!-- comment -->

Nous ajoutons ensuite des lignes verticales afin de mettre en évidence le nombre d’époques choisi.

```{r}
one_split_score <- one_split(score_torch)
(gg.torch.text <- gg.torch.point+
  geom_vline(aes(
    xintercept=best_epoch),
    data=one_split_score)+
  geom_text(aes(
    best_epoch, -Inf, label=paste0(" selected epoch=", best_epoch)),
    vjust=-0.5, hjust=0,
    data=one_split_score))
```

Ci-dessus, on peut voir que nous avons choisi le nombre d’époques qui minimise la perte logistique sur l’ensemble de validation.
<!-- comment -->
Nous proposons ensuite un résumé, qui affiche le nombre d’époques pour chaque jeu de données, division de validation croisée et algorithme d’apprentissage.

```{r}
(history_best_wide <- dcast(
  history_best,
  learner_id + set + task_id + iteration + task_it ~ measure,
  list(min, max, mid=function(x)(min(x)+max(x))/2),
  value.var="epoch"))
```

Le tableau présente des colonnes pour les époques correspondant aux meilleures mesures.
<!-- comment -->
Dans le graphique ci-dessous, nous utilisons les valeurs `mid`, soit à mi-chemin :

```{r}
history_best_valid <- history_best_wide[set=="validation"]
learner.colors <- c(
  torch_linear="grey",
  torch_dense_50="red")
(gg.best.valid <- ggplot()+
   theme_bw()+
   scale_fill_manual(values=learner.colors)+
   xlab("Epochs with best validation log loss")+
   ylab("Epochs with best validation AUC")+
   geom_point(aes(
     epoch_mid_logloss, epoch_mid_AUC, fill=learner_id),
     clickSelects="task_it", size=5,
     color_off="white", color="black",
     data=history_best_valid)+
   facet_wrap("task_id"))
```

Dans le graphique ci-dessus, on observe que :

<!-- comment -->

* pour certains jeux de données (`sonar` et `waveform`), la meilleure époque a des valeurs inférieures au maximum, ce qui suggère que le choix pour le nombre d’époques est adéquat (ni surapprentissage ni sous-apprentissage);
<!-- comment -->
* pour d’autres jeux de données (`vowel` et `zip`) la meilleure époque correspond au maximum (200 époques), ce qui suggère un sous-apprentissage (on peut augmenter le nombre d’époques pour minimiser davantage la perte sur l’ensemble de validation).

<!-- comment -->

Ensuite, nous rajoutons un segment pour afficher le min et max du nombre d’époques qui atteignent la meilleure AUC pour l’ensemble de validation.

```{r}
(gg.best.seg <- gg.best.valid+
   scale_color_manual(values=learner.colors)+
   geom_segment(aes(
     epoch_mid_logloss, epoch_min_AUC,
     xend=epoch_mid_logloss, yend=epoch_max_AUC,
     color=learner_id),
     size=3,
     clickSelects="task_it",
     data=history_best_valid))
```

On constate ci-dessus que plusieurs époques atteignent le maximum d’AUC dans les ensembles de validation de `vowel` et `zip`.
<!-- comment -->
Nous préparons maintenant un graphique pour afficher les détails, avec des valeurs relatives entre 0 et 1 (car les valeurs absolues sont très différentes d’un jeu de données à l’autre).

```{r}
norm01 <- function(x)(x-min(x))/(max(x)-min(x))
history_long[
, relative_values := norm01(value)
, by=.(iteration, task_id, learner_id, set, measure)]
gg.log.auc <- ggplot()+
   theme_bw()+
   facet_grid(measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   geom_line(aes(
     epoch, relative_values, color=set, group=set),
     showSelected="task_it",
     data=history_long)
```

Enfin, nous combinons les graphiques pour créer une visualisation interactive.

```{r vis-torch}
(vis.torch <- animint(
  overview=gg.best.seg+
    theme_animint(width=800, colspan=2, last_in_row=TRUE)+
    ggtitle("Select task and cross-validation iteration"),
  details=gg.log.auc+
    ggtitle("Selected learning curves")))
```

Ci-dessous nous voyons deux graphiques :

<!-- comment -->
* cliquez sur le graphique du haut pour sélectionner un jeu de données et une division de validation croisée;
<!-- comment -->
* •	observez le graphique du bas, il affiche les courbes d’apprentissage pour la sélection.

<!-- comment -->

Pour certains jeux de données, l’AUC atteint rapidement un maximum proche de 1, ce qui rend difficile à estimer à quel point elle s’en rapproche. 
<!-- comment -->
Pour visualiser ces détails, nous calculons l’inverse de l’AUC (1-AUC).
<!-- comment -->
La meilleure valeur de l’AUC est 1, mais la meilleure valeur pour l’inverse de l’AUC est 0.
<!-- comment -->
Nous pouvons alors utiliser l’échelle logarithmique pour visualiser la proximité entre la courbe et le 0.

```{r}
history_long[, let(
  Measure = factor(
    ifelse(measure=="logloss", "logloss", "InvAUC"),
    c("logloss","InvAUC")),
  Relative_values = ifelse(
    measure=="logloss", relative_values, 1-relative_values))]
gg.log.scale <- ggplot()+
   theme_bw()+
   facet_grid(Measure ~ learner_id, scales="free")+
   scale_x_continuous(breaks=seq(50,200,by=50))+
   scale_y_log10()+
   geom_line(aes(
     epoch, Relative_values, color=set, group=set),
     showSelected=c("task_it", "set"),
     data=history_long)
```

Notez que, dans le code ci-dessus, nous avons utilisé des majuscules pour les variables `Measure` et `Relative_values`, et une échelle logarithmique pour Y.
<!-- comment -->
Nous ajoutons ce graphique à la visualisation interactive.

```{r vis-torch-log}
vis.torch.log <- vis.torch
vis.torch.log$detailsLog <- gg.log.scale+
  ggtitle("Selected learning curves (log scale)")+
  theme(legend.position="none")
vis.torch.log
```

La visualisation contient un graphique en bas à droite, qui affiche l’échelle logarithmique pour la sélection.
<!-- comment -->
En cliquant sur `vowel`, on voit les différences entre les graphiques de détails (en bas à droite, il est plus facile de voir quelles époques sont les meilleures).

### glmnet {#glmnet}

Dans cette section, nous explorons des visualisations de l’algorithme `glmnet`, un modèle linéaire avec régularisation L1.
<!-- comment -->
Nous commençons avec la méthode `plot()` du premier modèle.

```{r}
score_glmnet <- score_dt[grep("glmnet",learner_id)]
L <- score_glmnet$learner[[1]]
library(glmnet)
plot(L$model)
score_glmnet[1, title(paste(task_id, iteration), line=3)]
```

Dans le graphique ci-dessus, nous voyons la perte logistique « Binomial Deviance » pour l’ensemble de validation, en fonction de la taille du modèle (nombre de coefficients non nuls en haut, négative log pénalité en bas).
<!-- comment -->
On voit une courbe qui descend, mais ne remonte pas, ce qui suggère un sous-apprentissage.
<!-- comment -->
Nous créons une version de ce graphique avec le code ci-dessous :

```{r}
cv_glmnet_one <- with(L$model, data.table(nzero, lambda, cvm, cvsd))
(gg.glmnet <- ggplot()+
   scale_y_continuous("Validation log loss")+
   geom_segment(aes(
     -log(lambda), cvm+cvsd,
     xend=-log(lambda), yend=cvm-cvsd),
     data=cv_glmnet_one)+
   geom_point(aes(
     -log(lambda), cvm, fill=nzero),
     data=cv_glmnet_one)+
   scale_fill_gradient(low="white", high="red"))
```

Ci-dessus nous voyons un ggplot qui ressemble au graphique précédant, avec la couleur indiquant le nombre de coefficients non nuls.
<!-- comment -->
Nous y ajoutons du texte pour souligner ce nombre pour plusieurs niveaux de régularisation.

```{r}
gg.glmnet+
  geom_text(aes(
    -log(lambda), Inf, label=nzero),
    vjust=1,
    data=cv_glmnet_one[seq(1, .N, by=5)])
```

Le graphique ci-dessus affiche le nombre de coefficients non nuls en haut, pour plusieurs niveaux de régularisation.
<!-- comment -->
L’objectif de cette section est de produire une visualisation interactive avec ces informations.
<!-- comment -->
Dans le code ci-dessous, nous calculons un tableau de résultats pour tous les modèles :

```{r}
(cv_glmnet_all <- score_glmnet[, with(learner[[1]]$model, data.table(
  nzero, lambda, cvm, cvsd
)), by=.(task_id, iteration, task_it)])
```

Le tableau ci-dessus contient une ligne par jeu de données, division de validation croisée et niveau de régularisation.
<!-- comment -->
Nous affichons ensuite la perte logistique sur l’ensemble de validation, avec une courbe pour chaque division de validation croisée et un panneau pour chaque jeu de données.

```{r}
ggplot()+
  scale_y_continuous("Validation log loss")+
  geom_line(aes(
    -log(lambda), cvm, group=iteration),
    data=cv_glmnet_all)+
  facet_wrap("task_id", scales="free")
```

Le graphique ci-dessus révèle que le modèle linéaire risque le surapprentissage dans trois jeux de données (`sonar`, `vowel` et `waveform`) si le niveau de régularisation est trop faible.
Dans les autres jeux de données (`spam` et `zip`), on voit une courbe qui ne remonte pas, signe de sous-apprentissage.
Un modèle non linéaire est susceptible de donner de meilleurs résultats.
Nous vérifions cette hypothèse en examinant les résultats précédents  :

* Dans `spam`, nous voyons que `torch_dense_50` obtient de meilleurs résultats que `cv_glmnet`. Nous en déduisons qu’une fonction linéaire n’est pas assez puissante pour obtenir la prédiction optimale dans ces données.
* Dans `zip`, nous voyons que tous les algorithmes d’apprentissage donnent une fonction de prédiction parfaite (ou presque). Nous en déduisons que ces données constituent un problème de prédiction facile à résoudre pour tous les algorithmes (même le modèle linéaire).

<!-- comment -->

## Résumé du chapitre et exercices {#ch20-exercises}

<!-- comment -->

Nous avons créé plusieurs visualisations des résultats d’apprentissage de `mlr3`.

<!-- comment -->

Exercices :

<!-- comment -->

* Dans `vis.select.fpr`, il n’y a pas de transitions fluides. Rajoutez-en une pour la variable `FPR`, et définissez `aes(key)` pour `geom_point()` et `geom_text()` dans `gg.tpr`.
<!-- comment -->
* Dans `vis.select.fpr`, un graphique affiche le meilleur taux de vrais positifs pour la sélection du taux de faux positifs. Supprimez ce graphique, et intégrez les informations au graphique des courbes ROC via `geom_point()` et `geom_label_aligned()`.
<!-- comment -->
* Le graphique `gg.summary` affiche seulement le max AUC. Ajoutez un `geom_segment()` qui affiche la variation d’AUC entre les algorithmes.
<!-- comment -->
* Le graphique `vis.select.fpr` affiche le meilleur taux de vrais positifs pour la sélection du taux de faux positifs.
<!-- comment -->
  * Ajoutez un graphique « Meilleur taux d’erreur pour la sélection du taux de faux positifs ». Indice : pour calculer le taux d’erreur, il faut faire une jointure entre le nombre d’étiquettes et les courbes ROC.
<!-- comment -->
  * Ajoutez un `geom_text()` qui affiche le taux de faux positifs sélectionné.
<!-- comment -->
  * Faites une visualisation qui affiche le meilleur taux de faux positifs pour une sélection du taux de vrais positifs.
<!-- comment -->
* Dans les légendes, les deux modèles linéaires (`cv_glmnet` et `torch_linear`) sont représentés par des couleurs différentes (jaune et violet). Harmonisez-les en leur attribuant des nuances d’une même couleur (par exemple, violet pâle et violet foncé).
<!-- comment -->
* Dans les visualisations des courbes ROC, rajoutez `aes(color)` ou `aes(fill)` aux autres graphiques, en adoptant une légende cohérente avec celle des courbes ROC. N’affichez qu’une légende dont les éléments sont triés dans l’ordre de présentation des axes Y affichant `learner_id`. Vérifiez qu’un clic sur un élément de la légende fait disparaître toutes les données correspondantes à cet algorithme d’apprentissage dans tous les graphiques.
<!-- comment -->
* Dans `vis.torch.log`, rajoutez des points pour mettre en évidence les meilleures époques et une ligne verticale pour indiquer l’époque choisie par la validation croisée.
* Chaque section du chapitre a présenté une méthode de visualisation. Combinez-en plusieurs dans une seule visualisation :
<!-- comment -->
  * Affichez les trois graphiques pour la vérification de la régularisation (`glmnet`, `torch` et nearest neighbors).
<!-- comment -->
  * Affichez les courbes ROC interactives à côté des graphiques pour la vérification de la régularisation.

<!-- comment -->

Consultez [l’annexe](/ch99) pour en apprendre davantage sur plusieurs méthodes de programmation R utiles pour la visualisation de données.

